{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install git+https://github.com/cair/tmu.git\n",
    "#%pip install pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tmu.models.classification.vanilla_classifier import TMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_sensors = [\"sensor_0_avg\", \"sensor_1_avg\", \"power_2_avg\", \"sensor_3_avg\", \"sensor_4_avg\", \"sensor_9_avg\", \"power_5_avg\", \"power_6_avg\", \"sensor_7_avg\", \"sensor_8_avg\", \"sensor_10_avg\", \"sensor_11_avg\"]\n",
    "\n",
    "def load_df_and_annotate_anomalies(farm, dataset_id):\n",
    "    path = f\"../../data/care_to_compare/Wind Farm {farm}/datasets/{dataset_id}.csv\"\n",
    "    df = pd.read_csv(path, delimiter=';')\n",
    "    \n",
    "    # If ['status_type_id'] is 0 or 2, then 0, else 1\n",
    "    df['label'] = df['status_type_id'].apply(lambda x: 0 if x in [0, 2] else 1)\n",
    "    \n",
    "    # Drop all columns except the ones in include_sensors\n",
    "    df = df[include_sensors + ['label', 'train_test']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all required data for training\n",
    "\n",
    "# 68;anomaly;2015-07-29 13:20:00;52063;2015-08-12 13:10:00;54076;Transformer failure\n",
    "\n",
    "# Load csv file\n",
    "df = pd.concat([\n",
    "    load_df_and_annotate_anomalies('C', 55), \n",
    "    load_df_and_annotate_anomalies('C', 81), \n",
    "    load_df_and_annotate_anomalies('C', 8),\n",
    "    load_df_and_annotate_anomalies('C', 85)\n",
    "])\n",
    "# Sensors to use\n",
    "\n",
    "train_data = df[df['train_test'] == 'train']\n",
    "test_data = df[df['train_test'] == 'prediction']\n",
    "\n",
    "X_train = train_data.drop(columns=['label', 'train_test'])\n",
    "y_train = train_data['label']\n",
    "\n",
    "X_test = test_data.drop(columns=['label', 'train_test'])\n",
    "y_test = test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all columns where the value is not a number\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "X_train = X_train.dropna(axis=1)\n",
    "\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "X_test = X_test.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that there are a equal numbers of rows of 0s and 1s\n",
    "n = min(y_train.value_counts())\n",
    "X_train = pd.concat([X_train[y_train == 0].sample(n=n), X_train[y_train == 1].sample(n=n)])\n",
    "y_train = pd.concat([y_train[y_train == 0].sample(n=n), y_train[y_train == 1].sample(n=n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 20899 0s and 20899 1s\n",
      "Test data: 8051 0s and 341 1s\n",
      "Index(['sensor_0_avg', 'sensor_1_avg', 'power_2_avg', 'sensor_3_avg',\n",
      "       'sensor_4_avg', 'sensor_9_avg', 'power_5_avg', 'power_6_avg',\n",
      "       'sensor_7_avg', 'sensor_8_avg', 'sensor_10_avg', 'sensor_11_avg'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print number of 0s and 1s in the label sets\n",
    "train_0s = np.count_nonzero(y_train == 0)\n",
    "train_1s = np.count_nonzero(y_train == 1)\n",
    "\n",
    "print(f\"Train data: {train_0s} 0s and {train_1s} 1s\")\n",
    "\n",
    "test_0s = np.count_nonzero(y_test == 0)\n",
    "test_1s = np.count_nonzero(y_test == 1)\n",
    "\n",
    "print(f\"Test data: {test_0s} 0s and {test_1s} 1s\")\n",
    "\n",
    "# PRint column names\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41798 120\n"
     ]
    }
   ],
   "source": [
    "def convert_to_10bit_integers(df):\n",
    "    normalized_df = df.copy()\n",
    "\n",
    "    for col in df.columns:\n",
    "        min_val = df[col].min()\n",
    "        max_val = df[col].max()\n",
    "\n",
    "        # Edge case wher all values are 0\n",
    "        if min_val == max_val:\n",
    "            normalized_df[col] = 0\n",
    "        else:\n",
    "            normalized_df[col] = ((df[col] - min_val) / (max_val - min_val) * 1023)\n",
    "\n",
    "    # Convert the normalized values to integers\n",
    "    int_df = normalized_df.astype(int)\n",
    "    \n",
    "    # Flatten each row into an array of 10-bit integers\n",
    "    int_arrays = int_df.apply(lambda row: row.values.flatten(), axis=1).tolist()\n",
    "    \n",
    "    # Represent each cell as a 10-bit integer string\n",
    "    bin_arrays = [[f\"{cell:010b}\" for cell in row] for row in int_arrays]\n",
    "\n",
    "    # Split each 10-bit integer string into individual integers for each row\n",
    "    # preserve the columns of bin_arrays\n",
    "    bin_int_arrays = [[int(cell) for cell in list(''.join(row))] for row in bin_arrays]\n",
    "\n",
    "    # Convert to numpy array\n",
    "    int_arrays = np.array(bin_int_arrays)\n",
    "\n",
    "    return int_arrays\n",
    "\n",
    "# Example usage\n",
    "X_train_binarized = convert_to_10bit_integers(X_train).astype(np.uint32)\n",
    "X_test_binarized = convert_to_10bit_integers(X_test).astype(np.uint32)\n",
    "\n",
    "y_train_binarized = y_train.values.astype(np.uint32)\n",
    "y_test_binarized = y_test.values.astype(np.uint32)\n",
    "\n",
    "# Print dimensions of the integer arrays\n",
    "print(len(X_train_binarized), len(X_train_binarized[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41798, 120)\n",
      "(41798,)\n",
      "(8392, 120)\n",
      "(8392,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_binarized.shape)\n",
    "print(y_train_binarized.shape)\n",
    "\n",
    "print(X_test_binarized.shape)    \n",
    "print(y_test_binarized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print each row into a file data.txt\n",
    "def write_to_file(X, y, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for i in range(len(X)):\n",
    "            f.write(\" \".join([str(x) for x in X[i]]) + \" \" + str(y[i]) + \"\\n\")\n",
    "\n",
    "write_to_file(X_train_binarized, y_train_binarized, \"data_train_exp_1.txt\")\n",
    "write_to_file(X_test_binarized, y_test_binarized, \"data_test_exp_1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running <class 'tmu.models.classification.vanilla_classifier.TMClassifier'> for 30 epochs\n",
      "Finished fitting\n",
      "Predicted 0s: 4248, Predicted 1s: 4144\n",
      "Epoch: 1, Accuracy: 54.15872\n",
      "Finished fitting\n",
      "Predicted 0s: 3715, Predicted 1s: 4677\n",
      "Epoch: 2, Accuracy: 48.14109\n",
      "Finished fitting\n",
      "Predicted 0s: 2614, Predicted 1s: 5778\n",
      "Epoch: 3, Accuracy: 34.87846\n",
      "Finished fitting\n",
      "Predicted 0s: 257, Predicted 1s: 8135\n",
      "Epoch: 4, Accuracy: 7.05434\n",
      "Finished fitting\n",
      "Predicted 0s: 1179, Predicted 1s: 7213\n",
      "Epoch: 5, Accuracy: 17.96949\n",
      "Finished fitting\n",
      "Predicted 0s: 2172, Predicted 1s: 6220\n",
      "Epoch: 6, Accuracy: 29.80219\n",
      "Finished fitting\n",
      "Predicted 0s: 393, Predicted 1s: 7999\n",
      "Epoch: 7, Accuracy: 8.67493\n",
      "Finished fitting\n",
      "Predicted 0s: 1586, Predicted 1s: 6806\n",
      "Epoch: 8, Accuracy: 22.81935\n",
      "Finished fitting\n",
      "Predicted 0s: 1421, Predicted 1s: 6971\n",
      "Epoch: 9, Accuracy: 20.87703\n",
      "Finished fitting\n",
      "Predicted 0s: 418, Predicted 1s: 7974\n",
      "Epoch: 10, Accuracy: 8.94900\n",
      "Finished fitting\n",
      "Predicted 0s: 2697, Predicted 1s: 5695\n",
      "Epoch: 11, Accuracy: 36.10582\n",
      "Finished fitting\n",
      "Predicted 0s: 1077, Predicted 1s: 7315\n",
      "Epoch: 12, Accuracy: 16.73022\n",
      "Finished fitting\n",
      "Predicted 0s: 1491, Predicted 1s: 6901\n",
      "Epoch: 13, Accuracy: 21.73499\n",
      "Finished fitting\n",
      "Predicted 0s: 2244, Predicted 1s: 6148\n",
      "Epoch: 14, Accuracy: 30.44566\n",
      "Finished fitting\n",
      "Predicted 0s: 4480, Predicted 1s: 3912\n",
      "Epoch: 15, Accuracy: 57.04242\n",
      "Finished fitting\n",
      "Predicted 0s: 700, Predicted 1s: 7692\n",
      "Epoch: 16, Accuracy: 12.23785\n",
      "Finished fitting\n",
      "Predicted 0s: 1172, Predicted 1s: 7220\n",
      "Epoch: 17, Accuracy: 17.88608\n",
      "Finished fitting\n",
      "Predicted 0s: 1275, Predicted 1s: 7117\n",
      "Epoch: 18, Accuracy: 19.04194\n",
      "Finished fitting\n",
      "Predicted 0s: 2255, Predicted 1s: 6137\n",
      "Epoch: 19, Accuracy: 30.76740\n",
      "Finished fitting\n",
      "Predicted 0s: 738, Predicted 1s: 7654\n",
      "Epoch: 20, Accuracy: 12.78599\n",
      "Finished fitting\n",
      "Predicted 0s: 423, Predicted 1s: 7969\n",
      "Epoch: 21, Accuracy: 8.96092\n",
      "Finished fitting\n",
      "Predicted 0s: 1466, Predicted 1s: 6926\n",
      "Epoch: 22, Accuracy: 21.41325\n",
      "Finished fitting\n",
      "Predicted 0s: 871, Predicted 1s: 7521\n",
      "Epoch: 23, Accuracy: 14.27550\n",
      "Finished fitting\n",
      "Predicted 0s: 1316, Predicted 1s: 7076\n",
      "Epoch: 24, Accuracy: 19.55434\n",
      "Finished fitting\n",
      "Predicted 0s: 1269, Predicted 1s: 7123\n",
      "Epoch: 25, Accuracy: 19.01811\n",
      "Finished fitting\n",
      "Predicted 0s: 1631, Predicted 1s: 6761\n",
      "Epoch: 26, Accuracy: 23.45091\n",
      "Finished fitting\n",
      "Predicted 0s: 2352, Predicted 1s: 6040\n",
      "Epoch: 27, Accuracy: 31.87560\n",
      "Finished fitting\n",
      "Predicted 0s: 3110, Predicted 1s: 5282\n",
      "Epoch: 28, Accuracy: 41.05100\n",
      "Finished fitting\n",
      "Predicted 0s: 2790, Predicted 1s: 5602\n",
      "Epoch: 29, Accuracy: 37.23785\n",
      "Finished fitting\n",
      "Predicted 0s: 2054, Predicted 1s: 6338\n",
      "Epoch: 30, Accuracy: 28.46759\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "tm = TMClassifier(\n",
    "    number_of_clauses=200,\n",
    "    T=200,\n",
    "    s=10.0,\n",
    "    max_included_literals=32,\n",
    "    weighted_clauses=True,\n",
    "    platform=\"CPU\",\n",
    "    batch_size=1000,\n",
    ")\n",
    "\n",
    "print(f\"Running {TMClassifier} for {epochs} epochs\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tm.fit(X_train_binarized, y_train_binarized)\n",
    "\n",
    "    print(\"Finished fitting\")\n",
    "    \n",
    "    pred = tm.predict(X_test_binarized)\n",
    "\n",
    "    pred_0s = np.count_nonzero(pred == 0)\n",
    "    pred_1s = np.count_nonzero(pred == 1)\n",
    "\n",
    "    print(f\"Predicted 0s: {pred_0s}, Predicted 1s: {pred_1s}\")\n",
    "  \n",
    "    result = 100* (pred == y_test_binarized).mean()\n",
    "\n",
    "    # Print every 20 epochs\n",
    "    #if (epoch + 1) % 20 == 0:\n",
    "    print(f\"Epoch: {epoch + 1}, Accuracy: {result:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"model.pickle\", \"wb\") as f:\n",
    "    pickle.dump(tm, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARE score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8392 120\n",
      "(8392, 120)\n"
     ]
    }
   ],
   "source": [
    "# Filter out all non\n",
    "eval_df = pd.concat([\n",
    "    load_df_and_annotate_anomalies('C', 55), \n",
    "    load_df_and_annotate_anomalies('C', 81), \n",
    "    load_df_and_annotate_anomalies('C', 8),\n",
    "    load_df_and_annotate_anomalies('C', 85)\n",
    "])\n",
    "\n",
    "eval_data = eval_df[eval_df['train_test'] == 'prediction']\n",
    "\n",
    "X_eval_data = eval_data.drop(columns=['label', 'train_test'])\n",
    "y_eval_data = eval_data['label']\n",
    "\n",
    "X_eval = convert_to_10bit_integers(X_eval_data).astype(np.uint32)\n",
    "y_eval = y_eval_data.values.astype(np.uint32)\n",
    "\n",
    "# Print dimensions of the integer arrays\n",
    "print(len(X_eval), len(X_eval[0]))\n",
    "\n",
    "# Print the size of the evaluation data\n",
    "print(X_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 28.46759\n",
      "[0 0 0 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# g = the ground truth of all data points with a normal status-ID within the prediction time frame\n",
    "g = y_eval\n",
    "\n",
    "# p = the corresponding prediction of an AD-model.\n",
    "p = tm.predict(X_eval)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = 100 * (p == g).mean()\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.5f}\")\n",
    "\n",
    "print(g)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP = 338, FN = 3, FP = 6000, TN = 2051, tot = 8392\n",
      "F0.5 = 0.0694187718\n"
     ]
    }
   ],
   "source": [
    "# Coverage\n",
    "# Detection of as many correct anomalies as possible\n",
    "\n",
    "beta = 0.5\n",
    "\n",
    "# the number of true positives based on g and p\n",
    "tp = np.sum((p == 1) & (g == 1))\n",
    "\n",
    "# the number of false negatives based on g and p\n",
    "fn = np.sum((p == 0) & (g == 1))\n",
    "\n",
    "# the number of false positives based on g and p\n",
    "fp = np.sum((p == 1) & (g == 0))\n",
    "\n",
    "tn = np.sum((p == 0) & (g == 0))\n",
    "\n",
    "Fbeta = (1 + beta**2) * tp / (1 + beta**2 * tp + beta**2 * fn + fp)\n",
    "\n",
    "print(f\"TP = {tp}, FN = {fn}, FP = {fp}, TN = {tn}, tot = {len(g)}\")\n",
    "print(f\"F{beta} = {Fbeta:.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "# Recognition of normal behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reliability\n",
    "# Few false alarm events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earliness\n",
    "# Detection of anomalies before fault gets critical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
