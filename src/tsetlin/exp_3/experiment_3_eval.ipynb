{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-06 19:04:32,595 - tmu.util.cuda_profiler - WARNING - Could not import pycuda: No module named 'pycuda'\n",
      "2025-01-06 19:04:32,597 - tmu.clause_bank.clause_bank_cuda - ERROR - No module named 'pycuda'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kjell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tmu\\clause_bank\\clause_bank_cuda.py\", line 41, in <module>\n",
      "    from pycuda._driver import Device, Context\n",
      "ModuleNotFoundError: No module named 'pycuda'\n",
      "2025-01-06 19:04:32,603 - tmu.clause_bank.clause_bank_cuda - WARNING - Could not import pycuda. This indicates that it is not installed! A possible fix is to run 'pip install pycuda'. Fallback to CPU ClauseBanks.\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from tmu.models.classification.vanilla_classifier import TMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_columns = [\"time_stamp\", \"asset_id\", \"id\", \"status_type_id\"], \n",
    "\n",
    "def load_df_and_annotate_anomalies(farm, dataset_id):\n",
    "    path = f\"../../../data/care_to_compare/Wind Farm {farm}/datasets/{dataset_id}.csv\"\n",
    "    df = pd.read_csv(path, delimiter=';')\n",
    "    \n",
    "    # If ['status_type_id'] is 0 or 2 (considered normal), then 0, else 1\n",
    "    df['label'] = df['status_type_id'].apply(lambda x: 0 if x in [0, 2] else 1)\n",
    "    \n",
    "    # Include all columns except for the ones in exclude_columns\n",
    "    df = df[[col for col in df.columns if col not in exclude_columns]]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_10bit_integers(df):\n",
    "    normalized_df = df.copy()\n",
    "\n",
    "    for col in df.columns:\n",
    "        min_val = df[col].min()\n",
    "        max_val = df[col].max()\n",
    "\n",
    "        # Edge case wher all values are 0\n",
    "        if min_val == max_val:\n",
    "            normalized_df[col] = 0\n",
    "        else:\n",
    "            normalized_df[col] = ((df[col] - min_val) / (max_val - min_val) * 1023)\n",
    "\n",
    "    # Convert the normalized values to integers\n",
    "    int_df = normalized_df.astype(int)\n",
    "    \n",
    "    # Flatten each row into an array of 10-bit integers\n",
    "    int_arrays = int_df.apply(lambda row: row.values.flatten(), axis=1).tolist()\n",
    "    \n",
    "    # Represent each cell as a 10-bit integer string\n",
    "    bin_arrays = [[f\"{cell:010b}\" for cell in row] for row in int_arrays]\n",
    "\n",
    "    # Split each 10-bit integer string into individual integers for each row\n",
    "    # preserve the columns of bin_arrays\n",
    "    bin_int_arrays = [[int(cell) for cell in list(''.join(row))] for row in bin_arrays]\n",
    "\n",
    "    # Convert to numpy array\n",
    "    int_arrays = np.array(bin_int_arrays)\n",
    "\n",
    "    return int_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "def load_model(filename) -> TMClassifier:\n",
    "    with open(filename, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = load_model(\"best.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARE score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_eval_metrics(farm, dataset_id, num_anom, num_norm, n_pred_anom, n_pred_norm, acc):\n",
    "    with open(\"eval_metrics.csv\", \"a\") as f:\n",
    "        f.write(f\"{farm},{dataset_id},{num_anom},{num_norm},{n_pred_anom},{n_pred_norm},{acc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(farm, dataset_id):\n",
    "    eval_data = load_df_and_annotate_anomalies(farm, dataset_id)\n",
    "\n",
    "    X_eval_data = eval_data.drop(columns=['label', 'train_test'])\n",
    "    y_eval_data = eval_data['label']\n",
    "\n",
    "    # Remove all columns where the value is not a number\n",
    "    X_eval_data = X_eval_data.apply(pd.to_numeric, errors='coerce')\n",
    "    X_eval_data = X_eval_data.dropna(axis=1)\n",
    "\n",
    "    X_eval = convert_to_10bit_integers(X_eval_data).astype(np.uint32)\n",
    "    y_eval = y_eval_data.values.astype(np.uint32)\n",
    "\n",
    "    # Print the number of 0s and 1s in the evaluation data\n",
    "    eval_0s = np.count_nonzero(y_eval == 0)\n",
    "    eval_1s = np.count_nonzero(y_eval == 1)\n",
    "\n",
    "    print(f\"Evaluation data: {eval_0s} 0s (normals) and {eval_1s} 1s (anomalies)\")\n",
    "\n",
    "    # g = the ground truth of all data points with a normal status-ID within the prediction time frame\n",
    "    g = y_eval\n",
    "\n",
    "    # p = the corresponding prediction of an AD-model.\n",
    "    p = tm.predict(X_eval)\n",
    "\n",
    "    print(f\"Normals: {np.count_nonzero(g == 1)}\")\n",
    "    print(f\"Anomalies: {np.count_nonzero(g == 0)}\")\n",
    "\n",
    "    # Accuracy\n",
    "\n",
    "    # the number of false positives based on g and p\n",
    "    fp = np.sum((p == 1) & (g == 0))\n",
    "\n",
    "    # the number of true negatives based on g and p\n",
    "    tn = np.sum((p == 0) & (g == 0))\n",
    "\n",
    "    acc = tn / (fp + tn)    \n",
    "\n",
    "    print(f\"Accuracy = {acc:.5f}\")\n",
    "\n",
    "    save_eval_metrics(farm, dataset_id, eval_0s, eval_1s, np.count_nonzero(p == 0), np.count_nonzero(p == 1), acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating C - 1\n",
      "Evaluation data: 47264 0s (normals) and 6305 1s (anomalies)\n",
      "Normals: 6305\n",
      "Anomalies: 47264\n",
      "Accuracy = 0.99998\n",
      "Evaluating C - 11\n",
      "Evaluation data: 46022 0s (normals) and 10415 1s (anomalies)\n",
      "Normals: 10415\n",
      "Anomalies: 46022\n",
      "Accuracy = 0.99998\n",
      "Evaluating C - 12\n",
      "Evaluation data: 52818 0s (normals) and 3289 1s (anomalies)\n",
      "Normals: 3289\n",
      "Anomalies: 52818\n",
      "Accuracy = 1.00000\n",
      "Evaluating C - 15\n",
      "Evaluation data: 48307 0s (normals) and 6126 1s (anomalies)\n",
      "Normals: 6126\n",
      "Anomalies: 48307\n",
      "Accuracy = 1.00000\n",
      "Evaluating C - 16\n",
      "Evaluation data: 45220 0s (normals) and 8348 1s (anomalies)\n",
      "Normals: 8348\n",
      "Anomalies: 45220\n",
      "Accuracy = 1.00000\n",
      "Evaluating C - 18\n",
      "Evaluation data: 48111 0s (normals) and 4737 1s (anomalies)\n",
      "Normals: 4737\n",
      "Anomalies: 48111\n",
      "Accuracy = 1.00000\n",
      "Evaluating C - 20\n",
      "Evaluation data: 49761 0s (normals) and 4240 1s (anomalies)\n",
      "Normals: 4240\n",
      "Anomalies: 49761\n",
      "Accuracy = 1.00000\n",
      "Evaluating C - 28\n",
      "Evaluation data: 47804 0s (normals) and 8114 1s (anomalies)\n",
      "Normals: 8114\n",
      "Anomalies: 47804\n",
      "Accuracy = 1.00000\n",
      "Evaluating C - 29\n",
      "Evaluation data: 43098 0s (normals) and 11767 1s (anomalies)\n",
      "Normals: 11767\n",
      "Anomalies: 43098\n",
      "Accuracy = 1.00000\n",
      "Evaluating C - 30\n",
      "Evaluation data: 50278 0s (normals) and 5833 1s (anomalies)\n",
      "Normals: 5833\n",
      "Anomalies: 50278\n",
      "Accuracy = 1.00000\n",
      "Evaluating C - 31\n",
      "Evaluation data: 49591 0s (normals) and 4998 1s (anomalies)\n",
      "Normals: 4998\n",
      "Anomalies: 49591\n",
      "Accuracy = 1.00000\n",
      "Evaluating C - 32\n",
      "Evaluation data: 49461 0s (normals) and 5548 1s (anomalies)\n",
      "Normals: 5548\n",
      "Anomalies: 49461\n",
      "Accuracy = 1.00000\n",
      "Evaluating C - 33\n",
      "Evaluation data: 49613 0s (normals) and 6260 1s (anomalies)\n",
      "Normals: 6260\n",
      "Anomalies: 49613\n",
      "Accuracy = 1.00000\n",
      "Evaluating C - 35\n",
      "Evaluation data: 47176 0s (normals) and 5439 1s (anomalies)\n",
      "Normals: 5439\n",
      "Anomalies: 47176\n",
      "Accuracy = 1.00000\n",
      "Evaluating C - 36\n",
      "Evaluation data: 49741 0s (normals) and 5707 1s (anomalies)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Read all filenames in ../../../data/care_to_compare/Wind Farm C/datasets\n",
    "filenames = os.listdir(\"../../../data/care_to_compare/Wind Farm C/datasets\")\n",
    "\n",
    "# Remove the .csv extension\n",
    "filenames = [filename.split(\".\")[0] for filename in filenames]\n",
    "\n",
    "\n",
    "for filename in filenames:\n",
    "    dataset_id = filename\n",
    "\n",
    "    print(f\"Evaluating C - {dataset_id}\")\n",
    "    evaluate(\"C\", dataset_id) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
