{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-07 18:50:17,712 - tmu.util.cuda_profiler - WARNING - Could not import pycuda: No module named 'pycuda'\n",
      "2025-01-07 18:50:17,713 - tmu.clause_bank.clause_bank_cuda - ERROR - No module named 'pycuda'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kjell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tmu\\clause_bank\\clause_bank_cuda.py\", line 41, in <module>\n",
      "    from pycuda._driver import Device, Context\n",
      "ModuleNotFoundError: No module named 'pycuda'\n",
      "2025-01-07 18:50:17,714 - tmu.clause_bank.clause_bank_cuda - WARNING - Could not import pycuda. This indicates that it is not installed! A possible fix is to run 'pip install pycuda'. Fallback to CPU ClauseBanks.\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from tmu.models.classification.vanilla_classifier import TMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_columns = [\"time_stamp\", \"asset_id\", \"id\", \"status_type_id\"], \n",
    "\n",
    "def load_df_and_annotate_anomalies(farm, event_id):\n",
    "    path = f\"../../../data/care_to_compare/Wind Farm {farm}/datasets/{event_id}.csv\"\n",
    "    df = pd.read_csv(path, delimiter=';')\n",
    "    \n",
    "    event_info = pd.read_csv(f\"../../../data/care_to_compare/Wind Farm {farm}/event_info.csv\", delimiter=';')\n",
    "\n",
    "    # Find the row where event_id = event_id\n",
    "    metadata = event_info[event_info['event_id'] == event_id]\n",
    "\n",
    "    event_label = metadata[\"event_label\"].values[0]\n",
    "    event_start_id = metadata[\"event_start_id\"].values[0]\n",
    "    event_end_id = metadata[\"event_end_id\"].values[0]\n",
    "\n",
    "    label_value = 1 if event_label == \"anomaly\" else 0\n",
    "\n",
    "    # All rows where the column \"id\" is between event_start_id and event_end_id\n",
    "    df['label'] = 0\n",
    "    df.loc[(df['id'] >= event_start_id) & (df['id'] <= event_end_id), 'label'] = label_value\n",
    "\n",
    "    # Include all columns except for the ones in exclude_columns\n",
    "    df = df[[col for col in df.columns if col not in exclude_columns]]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_10bit_integers(df):\n",
    "    normalized_df = df.copy()\n",
    "\n",
    "    for col in df.columns:\n",
    "        min_val = df[col].min()\n",
    "        max_val = df[col].max()\n",
    "\n",
    "        # Edge case wher all values are 0\n",
    "        if min_val == max_val:\n",
    "            normalized_df[col] = 0\n",
    "        else:\n",
    "            normalized_df[col] = ((df[col] - min_val) / (max_val - min_val) * 1023)\n",
    "\n",
    "    # Convert the normalized values to integers\n",
    "    int_df = normalized_df.astype(int)\n",
    "    \n",
    "    # Flatten each row into an array of 10-bit integers\n",
    "    int_arrays = int_df.apply(lambda row: row.values.flatten(), axis=1).tolist()\n",
    "    \n",
    "    # Represent each cell as a 10-bit integer string\n",
    "    bin_arrays = [[f\"{cell:010b}\" for cell in row] for row in int_arrays]\n",
    "\n",
    "    # Split each 10-bit integer string into individual integers for each row\n",
    "    # preserve the columns of bin_arrays\n",
    "    bin_int_arrays = [[int(cell) for cell in list(''.join(row))] for row in bin_arrays]\n",
    "\n",
    "    # Convert to numpy array\n",
    "    int_arrays = np.array(bin_int_arrays)\n",
    "\n",
    "    return int_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "def load_model(filename) -> TMClassifier:\n",
    "    with open(filename, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = load_model(\"best.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARE score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_eval_metrics(farm, dataset_id, num_anom, num_norm, n_pred_anom, n_pred_norm, acc):\n",
    "    with open(\"eval_metrics.csv\", \"a\") as f:\n",
    "        f.write(f\"{farm},{dataset_id},{num_anom},{num_norm},{n_pred_anom},{n_pred_norm},{acc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(farm, dataset_id):\n",
    "    eval_data = load_df_and_annotate_anomalies(farm, dataset_id)\n",
    "\n",
    "    X_eval_data = eval_data.drop(columns=['label', 'train_test'])\n",
    "    y_eval_data = eval_data['label']\n",
    "\n",
    "    # Remove all columns where the value is not a number\n",
    "    X_eval_data = X_eval_data.apply(pd.to_numeric, errors='coerce')\n",
    "    X_eval_data = X_eval_data.dropna(axis=1)\n",
    "\n",
    "    X_eval = convert_to_10bit_integers(X_eval_data).astype(np.uint32)\n",
    "    y_eval = y_eval_data.values.astype(np.uint32)\n",
    "\n",
    "    # Print the number of 0s and 1s in the evaluation data\n",
    "    eval_0s = np.count_nonzero(y_eval == 0)\n",
    "    eval_1s = np.count_nonzero(y_eval == 1)\n",
    "\n",
    "    print(f\"Evaluation data: {eval_0s} 0s (normals) and {eval_1s} 1s (anomalies)\")\n",
    "\n",
    "    # g = the ground truth of all data points with a normal status-ID within the prediction time frame\n",
    "    g = y_eval\n",
    "\n",
    "    # p = the corresponding prediction of an AD-model.\n",
    "    p = tm.predict(X_eval)\n",
    "\n",
    "    print(f\"Normals: {np.count_nonzero(g == 1)}\")\n",
    "    print(f\"Anomalies: {np.count_nonzero(g == 0)}\")\n",
    "\n",
    "    # Accuracy\n",
    "\n",
    "    # the number of false positives based on g and p\n",
    "    fp = np.sum((p == 1) & (g == 0))\n",
    "\n",
    "    # the number of true negatives based on g and p\n",
    "    tn = np.sum((p == 0) & (g == 0))\n",
    "\n",
    "    acc = tn / (fp + tn)    \n",
    "\n",
    "    print(f\"Accuracy = {acc:.5f}, FP = {fp}, TN = {tn}\")\n",
    "\n",
    "    save_eval_metrics(farm, dataset_id, eval_0s, eval_1s, np.count_nonzero(p == 0), np.count_nonzero(p == 1), acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating C - 1\n",
      "Evaluation data: 53569 0s (normals) and 0 1s (anomalies)\n",
      "Normals: 0\n",
      "Anomalies: 53569\n",
      "Accuracy = 0.99991\n",
      "Evaluating C - 11\n",
      "Evaluation data: 53280 0s (normals) and 3157 1s (anomalies)\n",
      "Normals: 3157\n",
      "Anomalies: 53280\n",
      "Accuracy = 0.99668\n",
      "Evaluating C - 12\n",
      "Evaluation data: 52848 0s (normals) and 3259 1s (anomalies)\n",
      "Normals: 3259\n",
      "Anomalies: 52848\n",
      "Accuracy = 1.00000\n",
      "Evaluating C - 15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m dataset_id \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating C - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(farm, dataset_id)\u001b[0m\n\u001b[0;32m      8\u001b[0m X_eval_data \u001b[38;5;241m=\u001b[39m X_eval_data\u001b[38;5;241m.\u001b[39mapply(pd\u001b[38;5;241m.\u001b[39mto_numeric, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m X_eval_data \u001b[38;5;241m=\u001b[39m X_eval_data\u001b[38;5;241m.\u001b[39mdropna(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m X_eval \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_10bit_integers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_eval_data\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint32)\n\u001b[0;32m     12\u001b[0m y_eval \u001b[38;5;241m=\u001b[39m y_eval_data\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint32)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Print the number of 0s and 1s in the evaluation data\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Read all filenames in ../../../data/care_to_compare/Wind Farm C/datasets\n",
    "filenames = os.listdir(\"../../../data/care_to_compare/Wind Farm C/datasets\")\n",
    "\n",
    "# Remove the .csv extension\n",
    "filenames = [filename.split(\".\")[0] for filename in filenames]\n",
    "\n",
    "\n",
    "for filename in filenames:\n",
    "    dataset_id = filename\n",
    "\n",
    "    print(f\"Evaluating C - {dataset_id}\")\n",
    "    evaluate(\"C\", int(dataset_id)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
