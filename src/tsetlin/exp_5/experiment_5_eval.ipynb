{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:18:28.882936Z",
     "start_time": "2025-02-12T14:18:28.846246Z"
    }
   },
   "source": [
    "# Import all required libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tmu.models.classification.vanilla_classifier import TMClassifier"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:19:28.076987Z",
     "start_time": "2025-02-12T14:19:28.070204Z"
    }
   },
   "source": [
    "## Helper functions for saving the model and accuracy\n",
    "\n",
    "# Helper function to load dataset\n",
    "def load_dataset(farm, event_id):\n",
    "    X = np.loadtxt(f\"./test_data/X_{farm}_{event_id}_10b.txt\", dtype=np.uint32)\n",
    "    X = np.array(X).astype(np.uint32)\n",
    "    return X\n",
    "\n",
    "\n",
    "def load_dataset_labels(farm, event_id):\n",
    "    y = np.loadtxt(f\"./test_data/y_{farm}_{event_id}_10b.txt\", dtype=np.uint32)\n",
    "    y = np.array(y).astype(np.uint32)\n",
    "    return y\n",
    "\n",
    "\n",
    "def load_dataset_statuses(farm, event_id):\n",
    "    z = np.loadtxt(f\"./test_data/z_{farm}_{event_id}_10b.txt\", dtype=np.uint32)\n",
    "    z = np.array(z).astype(np.uint32)\n",
    "    return z\n",
    "\n",
    "\n",
    "def load_model(filename) -> TMClassifier:\n",
    "    with open(filename, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:19:28.591604Z",
     "start_time": "2025-02-12T14:19:28.510665Z"
    }
   },
   "source": "tm = load_model(\"latest2.pkl\")",
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARE score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:19:29.491512Z",
     "start_time": "2025-02-12T14:19:29.488662Z"
    }
   },
   "source": [
    "def save_eval_metrics(farm, dataset_id, num_anom, num_norm, n_pred_anom, n_pred_norm, acc):\n",
    "    with open(\"eval_metrics.csv\", \"a\") as f:\n",
    "        f.write(f\"{farm},{dataset_id},{num_anom},{num_norm},{n_pred_anom},{n_pred_norm},{acc}\\n\")"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:19:29.960817Z",
     "start_time": "2025-02-12T14:19:29.957673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_accuracy(labels, predictions):\n",
    "    # g = the ground truth of all data points with a normal status-ID within the prediction time frame\n",
    "    g = labels\n",
    "\n",
    "    # p = the corresponding prediction of an AD-model.\n",
    "    p = predictions\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    fp = np.sum((p == 1) & (g == 0))\n",
    "\n",
    "    # the number of true negatives based on g and p\n",
    "    tn = np.sum((p == 0) & (g == 0))\n",
    "\n",
    "    accuracy = tn / (fp + tn)\n",
    "\n",
    "    return accuracy"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:19:30.327130Z",
     "start_time": "2025-02-12T14:19:30.323919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_coverage(labels, statuses, predictions):\n",
    "    beta = 0.5\n",
    "\n",
    "    g = labels\n",
    "    p = predictions\n",
    "\n",
    "    # the number of true positives based on g and p\n",
    "    tp = np.sum((p == 1) & (g == 1))\n",
    "\n",
    "    # the number of false negatives based on g and p\n",
    "    fn = np.sum((p == 0) & (g == 1))\n",
    "\n",
    "    # the number of false positives based on g and p\n",
    "    fp = np.sum((p == 1) & (g == 0))\n",
    "\n",
    "    beta_squared = beta ** 2\n",
    "\n",
    "    numerator = (1 + beta_squared) * tp\n",
    "    denominator = (1 + beta_squared) * tp + beta_squared * fn + fp\n",
    "\n",
    "    return numerator / denominator"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:19:30.561902Z",
     "start_time": "2025-02-12T14:19:30.556486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_reliability(labels, statuses, predictions):\n",
    "    s = [0 if x in [0, 2] else 1 for x in statuses]\n",
    "\n",
    "    p = predictions\n",
    "\n",
    "    N = len(s)\n",
    "\n",
    "    crit = [0] * (N + 1)  # Initialize crit array with zeros\n",
    "\n",
    "    for i in range(1, N + 1):\n",
    "        if s[i - 1] == 0:\n",
    "            if p[i - 1] == 1:\n",
    "                crit[i] = crit[i - 1] + 1\n",
    "            else:\n",
    "                crit[i] = max(crit[i - 1] - 1, 0)\n",
    "        else:\n",
    "            crit[i] = crit[i - 1]\n",
    "\n",
    "    criticality = crit[1:]\n",
    "\n",
    "    crit_max = np.max(criticality)\n",
    "\n",
    "    tc = 75\n",
    "\n",
    "    # If a value is larger than the threshold, then it is an anomaly (1) else it is not (0)\n",
    "    criticality = np.array([1 if c > tc else 0 for c in criticality])\n",
    "\n",
    "    return calculate_coverage(labels, criticality)"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:19:30.961453Z",
     "start_time": "2025-02-12T14:19:30.958294Z"
    }
   },
   "source": [
    "def evaluate(farm, dataset_id):\n",
    "    dataset = load_dataset(farm, dataset_id)\n",
    "    labels = load_dataset_labels(farm, dataset_id)\n",
    "    statuses = load_dataset_statuses(farm, dataset_id)\n",
    "\n",
    "    predictions = tm.predict(dataset)\n",
    "\n",
    "    coverage = calculate_coverage(labels, predictions)\n",
    "    accuracy = calculate_accuracy(labels, predictions)\n",
    "    reliability = calculate_reliability(labels, statuses, predictions)\n",
    "\n",
    "    return farm, dataset_id, coverage, accuracy, reliability, 0"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:19:57.064208Z",
     "start_time": "2025-02-12T14:19:31.825797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_datasets = [\n",
    "    55,  # 81, 47, 12, 4, 18, 28, 39, 66, 15, 78, 79, 30, 33, 11, 44,  # Has anomalies\n",
    "    # 8, 85, 6, 62, 36, 56, 94, 54, 43, 50, 64, 46, 65,\n",
    "]\n",
    "\n",
    "# 61, 93, 75, 41, 58, 48, 88, 57, 32, 89, 59, 63, 80, 37, 29, 1, 20, 60  # Without anomalies\n",
    "\n",
    "# Evaluate each dataset and visualize progress with tqdm\n",
    "results = [evaluate(\"C\", dataset) for dataset in tqdm(test_datasets)]\n",
    "\n",
    "# Each result is a tuple of (farm, dataset_id, coverage, accuracy, reliability, earliness)\n",
    "# Plot the accuracies for each dataset\n",
    "\n",
    "coverage_scores = [result[2] for result in results]\n",
    "accuracy_scores = [result[3] for result in results]\n",
    "reliability_scores = [result[4] for result in results]\n",
    "earliness_scores = [result[5] for result in results]\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:25<00:00, 25.23s/it]\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:20:01.641553Z",
     "start_time": "2025-02-12T14:20:01.437191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the average accuracy\n",
    "mean_coverage = np.mean(coverage_scores)\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "mean_reliability = np.mean(reliability_scores)\n",
    "mean_earliness = np.mean(earliness_scores)\n",
    "\n",
    "print(f\"Mean coverage: {mean_coverage:.5f}\")\n",
    "print(f\"Mean accuracy: {mean_accuracy:.5f}\")\n",
    "print(f\"Mean reliability: {mean_reliability:.5f}\")\n",
    "print(f\"Mean earliness: {mean_earliness:.5f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean coverage: 0.66980\n",
      "Mean accuracy: 0.98562\n",
      "Mean reliability: 0.51781\n",
      "Mean earliness: 0.00000\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T23:24:20.626543Z",
     "start_time": "2025-02-11T23:24:20.491952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(test_datasets, accuracies, color='skyblue')\n",
    "plt.xlabel(\"Dataset ID\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy for each dataset\")\n",
    "\n",
    "# Calculate the mean accuracy\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "\n",
    "plt.axhline(y=mean_accuracy, color='r', linestyle='-', label=f\"Mean accuracy: {mean_accuracy:.5f}\")\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m5\u001B[39m))\n\u001B[0;32m----> 2\u001B[0m plt\u001B[38;5;241m.\u001B[39mbar(test_datasets, \u001B[43maccuracies\u001B[49m, color\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mskyblue\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlabel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset ID\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m plt\u001B[38;5;241m.\u001B[39mylabel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'accuracies' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
