{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T15:07:19.700922Z",
     "start_time": "2025-04-23T15:07:19.698842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import all required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ],
   "id": "68f65dd8847de2f6",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T15:07:19.708633Z",
     "start_time": "2025-04-23T15:07:19.706207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(42)"
   ],
   "id": "c39f852fe0a113d1",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T15:07:19.728216Z",
     "start_time": "2025-04-23T15:07:19.722377Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def binarize_dataset_with_hdc(dataset, D=10000, Q=8, n_gram=3, show_details=False):\n",
    "    \"\"\"\n",
    "    Binarize a multi-feature dataset using Hyperdimensional Computing (HDC).\n",
    "\n",
    "    Parameters:\n",
    "        dataset (np.ndarray): Dataset with shape [num_samples, num_features]\n",
    "        D (int): Dimension of hypervectors\n",
    "        Q (int): Number of quantization levels\n",
    "        n_gram (int): Size of N-gram window for sequence encoding\n",
    "        show_details (bool): Whether to print detailed output\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of binary hypervectors, one for each sample\n",
    "    \"\"\"\n",
    "\n",
    "    num_samples, num_features = dataset.shape\n",
    "\n",
    "    if show_details:\n",
    "        print(f\"Dataset shape: {dataset.shape}\")\n",
    "        print(f\"Number of samples: {num_samples}\")\n",
    "        print(f\"Number of features: {num_features}\")\n",
    "\n",
    "    # HDC operations\n",
    "    def generate_random_hypervector(D):\n",
    "        \"\"\"Generate a random binary hypervector of dimension D\"\"\"\n",
    "        return np.random.randint(0, 2, D, dtype=np.uint8)\n",
    "\n",
    "    def bind(hv1, hv2):\n",
    "        \"\"\"Binding operation (XOR)\"\"\"\n",
    "        return np.logical_xor(hv1, hv2).astype(np.uint8)\n",
    "\n",
    "    def bundle(hvs):\n",
    "        \"\"\"Bundling operation (majority vote)\"\"\"\n",
    "        stacked = np.vstack(hvs)\n",
    "        counts = np.sum(stacked, axis=0)\n",
    "\n",
    "        # Majority voting\n",
    "        threshold = len(hvs) / 2\n",
    "        return (counts > threshold).astype(np.uint8)\n",
    "\n",
    "    # Find global min and max for each feature\n",
    "    min_values = np.min(dataset, axis=0)\n",
    "    max_values = np.max(dataset, axis=0)\n",
    "\n",
    "    # Generate feature ID vectors\n",
    "    feature_id_vectors = [generate_random_hypervector(D) for _ in range(num_features)]\n",
    "\n",
    "    # Generate interval vectors for each feature\n",
    "    # We'll use the same quantization levels for all features, but different random vectors\n",
    "    all_interval_vectors = []\n",
    "    for feature_idx in range(num_features):\n",
    "        feature_interval_vectors = [generate_random_hypervector(D) for _ in range(Q)]\n",
    "        all_interval_vectors.append(feature_interval_vectors)\n",
    "\n",
    "    # Generate position vectors for N-gram encoding\n",
    "    position_vectors = [generate_random_hypervector(D) for _ in range(n_gram)]\n",
    "\n",
    "    # Generate gram vectors\n",
    "    gram_vectors = [generate_random_hypervector(D) for _ in range(n_gram)]\n",
    "\n",
    "    # Binarize the dataset\n",
    "    binarized_samples = []\n",
    "\n",
    "    for sample_idx in range(num_samples):\n",
    "        # Get the feature vector for this sample\n",
    "        sample = dataset[sample_idx]\n",
    "\n",
    "        if show_details and sample_idx == 0:\n",
    "            print(f\"\\nProcessing sample 0: {sample}\")\n",
    "\n",
    "        # Encode each feature value into a hypervector\n",
    "        feature_hvs = []\n",
    "\n",
    "        for feature_idx in range(num_features):\n",
    "            value = sample[feature_idx]\n",
    "            min_val = min_values[feature_idx]\n",
    "            max_val = max_values[feature_idx]\n",
    "\n",
    "            # Skip features with no variation\n",
    "            if min_val == max_val:\n",
    "                continue\n",
    "\n",
    "            # Quantize the value\n",
    "            step = (max_val - min_val) / Q\n",
    "            bucket_idx = min(Q - 1, max(0, int((value - min_val) / step)))\n",
    "            value_hv = all_interval_vectors[feature_idx][bucket_idx]\n",
    "\n",
    "            # Bind the value HV with the feature ID to create a unique representation\n",
    "            feature_hv = bind(value_hv, feature_id_vectors[feature_idx])\n",
    "            feature_hvs.append(feature_hv)\n",
    "\n",
    "            if show_details and sample_idx == 0 and feature_idx < 3:\n",
    "                # Only show the first 3 features for the first sample\n",
    "                print(f\"  Feature {feature_idx} value: {value:.4f}, bucket: {bucket_idx}\")\n",
    "\n",
    "        # APPROACH 1: Bundle all feature hypervectors\n",
    "        # This is simpler but doesn't capture inter-feature relationships\n",
    "        if num_features <= n_gram:\n",
    "            # If we have fewer features than n_gram size, just bundle them\n",
    "            sample_hv = bundle(feature_hvs)\n",
    "        else:\n",
    "            # Apply N-gram encoding to capture relationships between adjacent features\n",
    "            ngram_hvs = []\n",
    "\n",
    "            for i in range(num_features - n_gram + 1):\n",
    "                gram_elements = []\n",
    "\n",
    "                for j in range(n_gram):\n",
    "                    feature_idx = i + j\n",
    "                    if feature_idx < len(feature_hvs):  # Check if within bounds\n",
    "                        hv = feature_hvs[feature_idx]\n",
    "\n",
    "                        # Bind with position vector\n",
    "                        pos_bound_hv = bind(hv, position_vectors[j])\n",
    "\n",
    "                        # Bind with gram vector\n",
    "                        bound_hv = bind(pos_bound_hv, gram_vectors[j])\n",
    "                        gram_elements.append(bound_hv)\n",
    "\n",
    "                # Bundle this N-gram\n",
    "                if gram_elements:  # Check if we have elements to bundle\n",
    "                    ngram_hv = bundle(gram_elements)\n",
    "                    ngram_hvs.append(ngram_hv)\n",
    "\n",
    "            # Bundle all N-grams to get final representation\n",
    "            if ngram_hvs:  # Check if we have N-grams to bundle\n",
    "                sample_hv = bundle(ngram_hvs)\n",
    "            else:\n",
    "                # Fallback: just bundle all feature vectors\n",
    "                sample_hv = bundle(feature_hvs)\n",
    "\n",
    "        binarized_samples.append(sample_hv)\n",
    "\n",
    "        if show_details and sample_idx == 0:\n",
    "            # Count ones in the first sample's binary vector\n",
    "            ones_count = np.sum(sample_hv)\n",
    "            print(\n",
    "                f\"  Sample 0 binary vector: {len(sample_hv)} bits, {ones_count} ones ({ones_count / len(sample_hv):.4f})\")\n",
    "\n",
    "    # Stack all sample hypervectors into a single array\n",
    "    binarized_dataset = np.vstack(binarized_samples)\n",
    "\n",
    "    if show_details:\n",
    "        print(f\"\\nBinarized dataset shape: {binarized_dataset.shape}\")\n",
    "\n",
    "    return binarized_dataset"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T15:07:19.733460Z",
     "start_time": "2025-04-23T15:07:19.731526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wind_farm = \"A\"\n",
    "train_datasets = [68, 22, 72, 73, 0, 26, 40, 42, 10, 45, 84, 25, 69, 13, 24, 3, 17, 38, 71, 14, 92, 51]\n",
    "test_datasets = [68, 22, 72, 73, 0, 26, 40, 42, 10, 45, 84, 25, 69, 13, 24, 3, 17, 38, 71, 14, 92, 51]\n"
   ],
   "id": "feaf8573a94e6196",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T15:07:19.742011Z",
     "start_time": "2025-04-23T15:07:19.740152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create folders data_test and data_train if they do not exist\n",
    "os.makedirs(\"data_test\", exist_ok=True)\n",
    "os.makedirs(\"data_train\", exist_ok=True)"
   ],
   "id": "f44feaaab5d59a69",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T15:07:19.748296Z",
     "start_time": "2025-04-23T15:07:19.745990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "exclude_columns = [\"time_stamp\", \"asset_id\", \"id\"]\n",
    "\n",
    "\n",
    "def load_df_and_annotate_anomalies(farm, event_id):\n",
    "    path = f\"../../../data/care_to_compare/Wind Farm {farm}/datasets/{event_id}.csv\"\n",
    "    df = pd.read_csv(path, delimiter=';')\n",
    "\n",
    "    # If status id is 1, 3, 4, 5, then it is an anomaly\n",
    "    # Add new label column\n",
    "\n",
    "    df['label'] = df['status_type_id'].apply(lambda x: 1 if x in [1, 3, 4, 5] else 0)\n",
    "\n",
    "    # Include all columns except for the ones in exclude_columns\n",
    "    df = df[[col for col in df.columns if col not in exclude_columns]]\n",
    "\n",
    "    # Replace inf values with NaN and drop rows with NaN values\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df"
   ],
   "id": "767812d8e26efd53",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T15:07:19.762577Z",
     "start_time": "2025-04-23T15:07:19.756749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def binarize_dataset_for_training(farm, event_id, output_path):\n",
    "    # Load original dataset from file\n",
    "    df = load_df_and_annotate_anomalies(farm, event_id)\n",
    "    df = df[df['train_test'] == 'train']\n",
    "\n",
    "    anomalies = df[df['label'] == 1]\n",
    "    normal = df[df['label'] == 0]\n",
    "\n",
    "    # Take only 1000 rows\n",
    "    if len(anomalies) > 3000:\n",
    "        anomalies = anomalies.sample(3000, random_state=42)\n",
    "\n",
    "    # Ensure that the dataset is balanced\n",
    "    if len(anomalies) > len(normal):\n",
    "        anomalies = anomalies.sample(len(normal), random_state=42)\n",
    "    else:\n",
    "        normal = normal.sample(len(anomalies), random_state=42)\n",
    "\n",
    "    df = pd.concat([anomalies, normal])\n",
    "\n",
    "    # Split into data and labels\n",
    "    X_values = df.drop(columns=['label', 'train_test', 'status_type_id'])\n",
    "    X_values = X_values.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Encode the sequence\n",
    "    final_binary_vector = binarize_dataset_with_hdc(X_values.values)\n",
    "\n",
    "    # Output to file using np\n",
    "    np.savetxt(f\"{output_path}/X_{farm}_{event_id}.txt\", final_binary_vector, fmt='%d')\n",
    "\n",
    "    label_df = pd.DataFrame({\n",
    "        'label': df['label'].values,\n",
    "        'status_type_id': df['status_type_id'].values,\n",
    "        'train_test': df['train_test'].values\n",
    "    })\n",
    "\n",
    "    label_df.to_csv(f\"{output_path}/y_{farm}_{event_id}.csv\", index=False)\n",
    "\n",
    "    print(f\"Done with {event_id}\")\n",
    "\n",
    "    # Calculate statistics\n",
    "    total_bits = len(final_binary_vector)\n",
    "\n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"Total rows: {total_bits}\")\n",
    "    print(f\"Total columns: {len(final_binary_vector[0])}\")\n",
    "\n",
    "\n",
    "def binarize_dataset_for_testing(farm, event_id, output_path):\n",
    "    # Load original dataset from file\n",
    "    df = load_df_and_annotate_anomalies(farm, event_id)\n",
    "\n",
    "    # Only take the data that is in the prediction set\n",
    "    df = df[df['train_test'] == 'prediction']\n",
    "\n",
    "    # Split into data and labels\n",
    "    X_values = df.drop(columns=['label', 'train_test', 'status_type_id'])\n",
    "    X_values = X_values.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Encode the sequence\n",
    "    final_binary_vector = binarize_dataset_with_hdc(X_values.values)\n",
    "\n",
    "    # Output to file using np\n",
    "    np.savetxt(f\"{output_path}/X_{farm}_{event_id}.txt\", final_binary_vector, fmt='%d')\n",
    "\n",
    "    label_df = pd.DataFrame({\n",
    "        'label': df['label'].values,\n",
    "        'status_type_id': df['status_type_id'].values,\n",
    "        'train_test': df['train_test'].values\n",
    "    })\n",
    "\n",
    "    label_df.to_csv(f\"{output_path}/y_{farm}_{event_id}.csv\", index=False)\n",
    "\n",
    "    # Calculate statistics\n",
    "    total_bits = len(final_binary_vector)\n",
    "\n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"Total rows: {total_bits}\")\n",
    "    print(f\"Total columns: {len(final_binary_vector[0])}\")\n"
   ],
   "id": "6c104fdf24752f99",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T15:10:30.104905Z",
     "start_time": "2025-04-23T15:07:19.771715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset in train_datasets:\n",
    "    binarize_dataset_for_training(wind_farm, dataset, \"./data_train\")"
   ],
   "id": "89841ee82e5285d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 68\n",
      "\n",
      "Statistics:\n",
      "Total rows: 6000\n",
      "Total columns: 10000\n",
      "Done with 22\n",
      "\n",
      "Statistics:\n",
      "Total rows: 6000\n",
      "Total columns: 10000\n",
      "Done with 72\n",
      "\n",
      "Statistics:\n",
      "Total rows: 6000\n",
      "Total columns: 10000\n",
      "Done with 73\n",
      "\n",
      "Statistics:\n",
      "Total rows: 6000\n",
      "Total columns: 10000\n",
      "Done with 0\n",
      "\n",
      "Statistics:\n",
      "Total rows: 6000\n",
      "Total columns: 10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m train_datasets:\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mbinarize_dataset_for_training\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwind_farm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./data_train\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[23], line 26\u001B[0m, in \u001B[0;36mbinarize_dataset_for_training\u001B[0;34m(farm, event_id, output_path)\u001B[0m\n\u001B[1;32m     23\u001B[0m X_values \u001B[38;5;241m=\u001B[39m X_values\u001B[38;5;241m.\u001B[39mapply(pd\u001B[38;5;241m.\u001B[39mto_numeric, errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcoerce\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Encode the sequence\u001B[39;00m\n\u001B[0;32m---> 26\u001B[0m final_binary_vector \u001B[38;5;241m=\u001B[39m \u001B[43mbinarize_dataset_with_hdc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_values\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Output to file using np\u001B[39;00m\n\u001B[1;32m     29\u001B[0m np\u001B[38;5;241m.\u001B[39msavetxt(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutput_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/X_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfarm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mevent_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m, final_binary_vector, fmt\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[19], line 125\u001B[0m, in \u001B[0;36mbinarize_dataset_with_hdc\u001B[0;34m(dataset, D, Q, n_gram, show_details)\u001B[0m\n\u001B[1;32m    123\u001B[0m     \u001B[38;5;66;03m# Bundle this N-gram\u001B[39;00m\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gram_elements:  \u001B[38;5;66;03m# Check if we have elements to bundle\u001B[39;00m\n\u001B[0;32m--> 125\u001B[0m         ngram_hv \u001B[38;5;241m=\u001B[39m \u001B[43mbundle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgram_elements\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    126\u001B[0m         ngram_hvs\u001B[38;5;241m.\u001B[39mappend(ngram_hv)\n\u001B[1;32m    128\u001B[0m \u001B[38;5;66;03m# Bundle all N-grams to get final representation\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[19], line 38\u001B[0m, in \u001B[0;36mbinarize_dataset_with_hdc.<locals>.bundle\u001B[0;34m(hvs)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Bundling operation (majority vote)\"\"\"\u001B[39;00m\n\u001B[1;32m     37\u001B[0m stacked \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mvstack(hvs)\n\u001B[0;32m---> 38\u001B[0m counts \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstacked\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Majority voting\u001B[39;00m\n\u001B[1;32m     41\u001B[0m threshold \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(hvs) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m\n",
      "File \u001B[0;32m~/Documents/GitHub/uia-master-thesis/.venv/lib/python3.12/site-packages/numpy/core/fromnumeric.py:2313\u001B[0m, in \u001B[0;36msum\u001B[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m   2310\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[1;32m   2311\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\u001B[0;32m-> 2313\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapreduction\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2314\u001B[0m \u001B[43m                      \u001B[49m\u001B[43minitial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/uia-master-thesis/.venv/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88\u001B[0m, in \u001B[0;36m_wrapreduction\u001B[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001B[0m\n\u001B[1;32m     85\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     86\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m reduction(axis\u001B[38;5;241m=\u001B[39maxis, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpasskwargs)\n\u001B[0;32m---> 88\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mufunc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpasskwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T15:16:04.228349Z",
     "start_time": "2025-04-23T15:10:32.554128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset in test_datasets:\n",
    "    binarize_dataset_for_testing(wind_farm, dataset, \"./data_test\")"
   ],
   "id": "a4f5f329094e459e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics:\n",
      "Total rows: 2295\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1147\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2017\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1729\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2837\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1441\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 4937\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1583\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1269\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1440\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1437\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2423\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2593\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 3583\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2714\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 3302\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2781\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2544\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2449\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1901\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1774\n",
      "Total columns: 10000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2393\n",
      "Total columns: 10000\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
