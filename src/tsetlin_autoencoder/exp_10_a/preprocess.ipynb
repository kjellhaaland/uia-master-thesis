{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:19:17.521311Z",
     "start_time": "2025-04-16T17:19:17.518839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import all required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ],
   "id": "68f65dd8847de2f6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:19:17.533538Z",
     "start_time": "2025-04-16T17:19:17.529594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(42)"
   ],
   "id": "c39f852fe0a113d1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-16T17:19:17.620073Z",
     "start_time": "2025-04-16T17:19:17.611003Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def binarize_dataset_with_hdc(dataset, D=2000, Q=8, n_gram=3, show_details=False):\n",
    "    \"\"\"\n",
    "    Binarize a multi-feature dataset using Hyperdimensional Computing (HDC).\n",
    "\n",
    "    Parameters:\n",
    "        dataset (np.ndarray): Dataset with shape [num_samples, num_features]\n",
    "        D (int): Dimension of hypervectors\n",
    "        Q (int): Number of quantization levels\n",
    "        n_gram (int): Size of N-gram window for sequence encoding\n",
    "        show_details (bool): Whether to print detailed output\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of binary hypervectors, one for each sample\n",
    "    \"\"\"\n",
    "\n",
    "    num_samples, num_features = dataset.shape\n",
    "\n",
    "    if show_details:\n",
    "        print(f\"Dataset shape: {dataset.shape}\")\n",
    "        print(f\"Number of samples: {num_samples}\")\n",
    "        print(f\"Number of features: {num_features}\")\n",
    "\n",
    "    # HDC operations\n",
    "    def generate_random_hypervector(D):\n",
    "        \"\"\"Generate a random binary hypervector of dimension D\"\"\"\n",
    "        return np.random.randint(0, 2, D, dtype=np.uint8)\n",
    "\n",
    "    def bind(hv1, hv2):\n",
    "        \"\"\"Binding operation (XOR)\"\"\"\n",
    "        return np.logical_xor(hv1, hv2).astype(np.uint8)\n",
    "\n",
    "    def bundle(hvs):\n",
    "        \"\"\"Bundling operation (majority vote)\"\"\"\n",
    "        stacked = np.vstack(hvs)\n",
    "        counts = np.sum(stacked, axis=0)\n",
    "\n",
    "        # Majority voting\n",
    "        threshold = len(hvs) / 2\n",
    "        return (counts > threshold).astype(np.uint8)\n",
    "\n",
    "    # Find global min and max for each feature\n",
    "    min_values = np.min(dataset, axis=0)\n",
    "    max_values = np.max(dataset, axis=0)\n",
    "\n",
    "    # Generate feature ID vectors\n",
    "    feature_id_vectors = [generate_random_hypervector(D) for _ in range(num_features)]\n",
    "\n",
    "    # Generate interval vectors for each feature\n",
    "    # We'll use the same quantization levels for all features, but different random vectors\n",
    "    all_interval_vectors = []\n",
    "    for feature_idx in range(num_features):\n",
    "        feature_interval_vectors = [generate_random_hypervector(D) for _ in range(Q)]\n",
    "        all_interval_vectors.append(feature_interval_vectors)\n",
    "\n",
    "    # Generate position vectors for N-gram encoding\n",
    "    position_vectors = [generate_random_hypervector(D) for _ in range(n_gram)]\n",
    "\n",
    "    # Generate gram vectors\n",
    "    gram_vectors = [generate_random_hypervector(D) for _ in range(n_gram)]\n",
    "\n",
    "    # Binarize the dataset\n",
    "    binarized_samples = []\n",
    "\n",
    "    for sample_idx in range(num_samples):\n",
    "        # Get the feature vector for this sample\n",
    "        sample = dataset[sample_idx]\n",
    "\n",
    "        if show_details and sample_idx == 0:\n",
    "            print(f\"\\nProcessing sample 0: {sample}\")\n",
    "\n",
    "        # Encode each feature value into a hypervector\n",
    "        feature_hvs = []\n",
    "\n",
    "        for feature_idx in range(num_features):\n",
    "            value = sample[feature_idx]\n",
    "            min_val = min_values[feature_idx]\n",
    "            max_val = max_values[feature_idx]\n",
    "\n",
    "            # Skip features with no variation\n",
    "            if min_val == max_val:\n",
    "                continue\n",
    "\n",
    "            # Quantize the value\n",
    "            step = (max_val - min_val) / Q\n",
    "            bucket_idx = min(Q - 1, max(0, int((value - min_val) / step)))\n",
    "            value_hv = all_interval_vectors[feature_idx][bucket_idx]\n",
    "\n",
    "            # Bind the value HV with the feature ID to create a unique representation\n",
    "            feature_hv = bind(value_hv, feature_id_vectors[feature_idx])\n",
    "            feature_hvs.append(feature_hv)\n",
    "\n",
    "            if show_details and sample_idx == 0 and feature_idx < 3:\n",
    "                # Only show the first 3 features for the first sample\n",
    "                print(f\"  Feature {feature_idx} value: {value:.4f}, bucket: {bucket_idx}\")\n",
    "\n",
    "        # APPROACH 1: Bundle all feature hypervectors\n",
    "        # This is simpler but doesn't capture inter-feature relationships\n",
    "        if num_features <= n_gram:\n",
    "            # If we have fewer features than n_gram size, just bundle them\n",
    "            sample_hv = bundle(feature_hvs)\n",
    "        else:\n",
    "            # Apply N-gram encoding to capture relationships between adjacent features\n",
    "            ngram_hvs = []\n",
    "\n",
    "            for i in range(num_features - n_gram + 1):\n",
    "                gram_elements = []\n",
    "\n",
    "                for j in range(n_gram):\n",
    "                    feature_idx = i + j\n",
    "                    if feature_idx < len(feature_hvs):  # Check if within bounds\n",
    "                        hv = feature_hvs[feature_idx]\n",
    "\n",
    "                        # Bind with position vector\n",
    "                        pos_bound_hv = bind(hv, position_vectors[j])\n",
    "\n",
    "                        # Bind with gram vector\n",
    "                        bound_hv = bind(pos_bound_hv, gram_vectors[j])\n",
    "                        gram_elements.append(bound_hv)\n",
    "\n",
    "                # Bundle this N-gram\n",
    "                if gram_elements:  # Check if we have elements to bundle\n",
    "                    ngram_hv = bundle(gram_elements)\n",
    "                    ngram_hvs.append(ngram_hv)\n",
    "\n",
    "            # Bundle all N-grams to get final representation\n",
    "            if ngram_hvs:  # Check if we have N-grams to bundle\n",
    "                sample_hv = bundle(ngram_hvs)\n",
    "            else:\n",
    "                # Fallback: just bundle all feature vectors\n",
    "                sample_hv = bundle(feature_hvs)\n",
    "\n",
    "        binarized_samples.append(sample_hv)\n",
    "\n",
    "        if show_details and sample_idx == 0:\n",
    "            # Count ones in the first sample's binary vector\n",
    "            ones_count = np.sum(sample_hv)\n",
    "            print(\n",
    "                f\"  Sample 0 binary vector: {len(sample_hv)} bits, {ones_count} ones ({ones_count / len(sample_hv):.4f})\")\n",
    "\n",
    "    # Stack all sample hypervectors into a single array\n",
    "    binarized_dataset = np.vstack(binarized_samples)\n",
    "\n",
    "    if show_details:\n",
    "        print(f\"\\nBinarized dataset shape: {binarized_dataset.shape}\")\n",
    "\n",
    "    return binarized_dataset"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:19:17.637024Z",
     "start_time": "2025-04-16T17:19:17.633208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wind_farm = \"A\"\n",
    "train_datasets = [68, 22]\n",
    "test_datasets = [68, 22, 72, 73, 0, 26, 40, 42, 10, 45, 84, 25, 69, 13, 24, 3, 17, 38, 71, 14, 92, 51]\n"
   ],
   "id": "feaf8573a94e6196",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:19:17.658640Z",
     "start_time": "2025-04-16T17:19:17.651668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create folders data_test and data_train if they do not exist\n",
    "os.makedirs(\"data_test\", exist_ok=True)\n",
    "os.makedirs(\"data_train\", exist_ok=True)"
   ],
   "id": "f44feaaab5d59a69",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:19:17.678580Z",
     "start_time": "2025-04-16T17:19:17.669564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "exclude_columns = [\"time_stamp\", \"asset_id\", \"id\"]\n",
    "\n",
    "\n",
    "def load_df_and_annotate_anomalies(farm, event_id):\n",
    "    path = f\"../../../data/care_to_compare/Wind Farm {farm}/datasets/{event_id}.csv\"\n",
    "    df = pd.read_csv(path, delimiter=';')\n",
    "\n",
    "    event_info = pd.read_csv(f\"../../../data/care_to_compare/Wind Farm {farm}/event_info.csv\", delimiter=';')\n",
    "\n",
    "    # Find the row where event_id = event_id\n",
    "    metadata = event_info[event_info['event_id'] == event_id]\n",
    "\n",
    "    event_label = metadata[\"event_label\"].values[0]\n",
    "    event_start_id = metadata[\"event_start_id\"].values[0]\n",
    "    event_end_id = metadata[\"event_end_id\"].values[0]\n",
    "\n",
    "    label_value = 1 if event_label == \"anomaly\" else 0\n",
    "\n",
    "    # All rows where the column \"id\" is between event_start_id and event_end_id\n",
    "    df['label'] = 0\n",
    "    df.loc[(df['id'] >= event_start_id) & (df['id'] <= event_end_id), 'label'] = label_value\n",
    "\n",
    "    # Include all columns except for the ones in exclude_columns\n",
    "    df = df[[col for col in df.columns if col not in exclude_columns]]\n",
    "\n",
    "    # Replace inf values with NaN and drop rows with NaN values\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df"
   ],
   "id": "767812d8e26efd53",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:19:17.707209Z",
     "start_time": "2025-04-16T17:19:17.698923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def binarize_dataset_for_training(farm, event_id, output_path):\n",
    "    # Load original dataset from file\n",
    "    df = load_df_and_annotate_anomalies(farm, event_id)\n",
    "    df = df[df['train_test'] == 'train']\n",
    "\n",
    "    # Take only 1000 rows\n",
    "    #df = df[:1000]\n",
    "\n",
    "    df = df[df['status_type_id'].isin([0, 2])]\n",
    "\n",
    "    # Split into data and labels\n",
    "    X_values = df.drop(columns=['label', 'train_test', 'status_type_id'])\n",
    "    X_values = X_values.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Encode the sequence\n",
    "    final_binary_vector = binarize_dataset_with_hdc(X_values.values)\n",
    "\n",
    "    # Output to file using np\n",
    "    np.savetxt(f\"{output_path}/X_{farm}_{event_id}.txt\", final_binary_vector, fmt='%d')\n",
    "\n",
    "    label_df = pd.DataFrame({\n",
    "        'label': df['label'].values,\n",
    "        'status_type_id': df['status_type_id'].values,\n",
    "        'train_test': df['train_test'].values\n",
    "    })\n",
    "\n",
    "    label_df.to_csv(f\"{output_path}/y_{farm}_{event_id}.csv\", index=False)\n",
    "\n",
    "    print(f\"Done with {event_id}\")\n",
    "\n",
    "    # Calculate statistics\n",
    "    number_of_0s = np.sum(final_binary_vector == 0)\n",
    "    number_of_1s = np.sum(final_binary_vector == 1)\n",
    "\n",
    "    total_bits = len(final_binary_vector)\n",
    "\n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"Total bits: {total_bits}\")\n",
    "    print(f\"Number of 1s: {number_of_1s}\")\n",
    "    print(f\"Number of 0s: {number_of_0s}\")\n",
    "\n",
    "\n",
    "def binarize_dataset_for_testing(farm, event_id, output_path):\n",
    "    # Load original dataset from file\n",
    "    df = load_df_and_annotate_anomalies(farm, event_id)\n",
    "\n",
    "    # Only take the data that is in the prediction set\n",
    "    df = df[df['train_test'] == 'prediction']\n",
    "\n",
    "    # Split into data and labels\n",
    "    X_values = df.drop(columns=['label', 'train_test', 'status_type_id'])\n",
    "    X_values = X_values.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Encode the sequence\n",
    "    final_binary_vector = binarize_dataset_with_hdc(X_values.values)\n",
    "\n",
    "    # Output to file using np\n",
    "    np.savetxt(f\"{output_path}/X_{farm}_{event_id}.txt\", final_binary_vector, fmt='%d')\n",
    "\n",
    "    label_df = pd.DataFrame({\n",
    "        'label': df['label'].values,\n",
    "        'status_type_id': df['status_type_id'].values,\n",
    "        'train_test': df['train_test'].values\n",
    "    })\n",
    "\n",
    "    label_df.to_csv(f\"{output_path}/y_{farm}_{event_id}.csv\", index=False)\n",
    "\n",
    "    # Calculate statistics\n",
    "    total_bits = len(final_binary_vector)\n",
    "\n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"Total rows: {total_bits}\")\n",
    "    print(f\"Total columns: {len(final_binary_vector[0])}\")\n"
   ],
   "id": "6c104fdf24752f99",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:21:56.007296Z",
     "start_time": "2025-04-16T17:19:17.716555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset in train_datasets:\n",
    "    binarize_dataset_for_training(wind_farm, dataset, \"./data_train\")"
   ],
   "id": "89841ee82e5285d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 68\n",
      "\n",
      "Statistics:\n",
      "Total bits: 40437\n",
      "Number of 1s: 38101354\n",
      "Number of 0s: 42772646\n",
      "Done with 22\n",
      "\n",
      "Statistics:\n",
      "Total bits: 40804\n",
      "Number of 1s: 38290357\n",
      "Number of 0s: 43317643\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:24:38.087947Z",
     "start_time": "2025-04-16T17:24:35.772479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset in test_datasets:\n",
    "    binarize_dataset_for_testing(wind_farm, dataset, \"./data_test\")"
   ],
   "id": "a4f5f329094e459e",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m test_datasets:\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mbinarize_dataset_for_testing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwind_farm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./data_test\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[7], line 55\u001B[0m, in \u001B[0;36mbinarize_dataset_for_testing\u001B[0;34m(farm, event_id, output_path)\u001B[0m\n\u001B[1;32m     52\u001B[0m X_values \u001B[38;5;241m=\u001B[39m X_values\u001B[38;5;241m.\u001B[39mapply(pd\u001B[38;5;241m.\u001B[39mto_numeric, errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcoerce\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;66;03m# Encode the sequence\u001B[39;00m\n\u001B[0;32m---> 55\u001B[0m final_binary_vector \u001B[38;5;241m=\u001B[39m \u001B[43mbinarize_dataset_with_hdc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_values\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# Output to file using np\u001B[39;00m\n\u001B[1;32m     58\u001B[0m np\u001B[38;5;241m.\u001B[39msavetxt(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutput_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/X_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfarm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mevent_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m, final_binary_vector, fmt\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[3], line 117\u001B[0m, in \u001B[0;36mbinarize_dataset_with_hdc\u001B[0;34m(dataset, D, Q, n_gram, show_details)\u001B[0m\n\u001B[1;32m    114\u001B[0m hv \u001B[38;5;241m=\u001B[39m feature_hvs[feature_idx]\n\u001B[1;32m    116\u001B[0m \u001B[38;5;66;03m# Bind with position vector\u001B[39;00m\n\u001B[0;32m--> 117\u001B[0m pos_bound_hv \u001B[38;5;241m=\u001B[39m \u001B[43mbind\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_vectors\u001B[49m\u001B[43m[\u001B[49m\u001B[43mj\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;66;03m# Bind with gram vector\u001B[39;00m\n\u001B[1;32m    120\u001B[0m bound_hv \u001B[38;5;241m=\u001B[39m bind(pos_bound_hv, gram_vectors[j])\n",
      "Cell \u001B[0;32mIn[3], line 33\u001B[0m, in \u001B[0;36mbinarize_dataset_with_hdc.<locals>.bind\u001B[0;34m(hv1, hv2)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbind\u001B[39m(hv1, hv2):\n\u001B[1;32m     32\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Binding operation (XOR)\"\"\"\u001B[39;00m\n\u001B[0;32m---> 33\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogical_xor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhv1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhv2\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39muint8)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ff1909a19ec238ac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
