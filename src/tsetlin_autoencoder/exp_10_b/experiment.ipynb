{
 "cells": [
  {
   "cell_type": "code",
   "id": "e28183c3-d395-4a3a-8eb4-dba55cb1d527",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T10:51:18.938956Z",
     "start_time": "2025-04-16T10:51:18.931523Z"
    }
   },
   "source": [
    "#%pip install git+https://github.com/cair/tmu.git\n",
    "#%pip install numpy==1.26.4"
   ],
   "outputs": [],
   "execution_count": 122
  },
  {
   "cell_type": "code",
   "id": "2b757c20539f2247",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T10:51:18.947542Z",
     "start_time": "2025-04-16T10:51:18.944235Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from tmu.models.autoencoder.autoencoder import TMAutoEncoder"
   ],
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T10:51:18.961835Z",
     "start_time": "2025-04-16T10:51:18.959654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create folder models if it does not exist\n",
    "os.makedirs(\"models\", exist_ok=True)"
   ],
   "id": "30305fc8025f45c1",
   "outputs": [],
   "execution_count": 124
  },
  {
   "cell_type": "code",
   "id": "28162fc330bc11f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T10:51:18.978427Z",
     "start_time": "2025-04-16T10:51:18.974622Z"
    }
   },
   "source": [
    "def load_train_dataset(farm, event_id):\n",
    "    X = np.loadtxt(f\"./data_train/X_{farm}_{event_id}.txt\", dtype=np.uint32)\n",
    "    X = np.array(X).astype(np.uint32)\n",
    "    return X\n",
    "\n",
    "\n",
    "def load_test_dataset(farm, event_id):\n",
    "    X = np.loadtxt(f\"./data_test/X_{farm}_{event_id}.txt\", dtype=np.uint32)\n",
    "    X = np.array(X).astype(np.uint32)\n",
    "\n",
    "    X = X[:1000]  # Take the first 1000 rows\n",
    "\n",
    "    return X\n"
   ],
   "outputs": [],
   "execution_count": 125
  },
  {
   "cell_type": "code",
   "id": "da4ff95f-ff25-4203-89c8-7e6046e4356b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T10:51:18.985366Z",
     "start_time": "2025-04-16T10:51:18.982071Z"
    }
   },
   "source": [
    "def save_model(tm: TMAutoEncoder, filename: str):\n",
    "    a, d = tm.X_train, tm.encoded_X_train\n",
    "\n",
    "    tm.X_train = None\n",
    "    tm.encoded_X_train = None\n",
    "\n",
    "    with open(f\"./models/{filename}\", \"wb\") as f:\n",
    "        pickle.dump(tm, f)\n",
    "\n",
    "    tm.X_train = a\n",
    "    tm.encoded_X_train = d"
   ],
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T10:51:22.101125Z",
     "start_time": "2025-04-16T10:51:18.994710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_datasets = [34, 7]  #, 7]  #, 53, 27, 19, ]  # 77, 83, 52, 21, 2, 23, 87, 74, 86, 82]\n",
    "test_dataset = [53]\n",
    "\n",
    "X_train = np.concatenate([load_train_dataset(\"B\", i) for i in train_datasets])\n",
    "\n",
    "X_test = np.concatenate([load_test_dataset(\"B\", i) for i in test_dataset])\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ],
   "id": "a31d2f9342f1de77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (93206, 2000)\n",
      "X_test shape: (1000, 2000)\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T10:51:22.107882Z",
     "start_time": "2025-04-16T10:51:22.105784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hamming_loss(pred, X_test):\n",
    "    \"\"\"\n",
    "    Computes the Hamming loss between predicted and ground truth binary arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - pred (numpy array): Binary predictions of shape (n_samples, n_bits).\n",
    "    - X_test (numpy array): Ground truth binary values of shape (n_samples, n_bits).\n",
    "\n",
    "    Returns:\n",
    "    - float: Hamming loss (fraction of incorrect bits).\n",
    "    \"\"\"\n",
    "    assert pred.shape == X_test.shape, \"Shapes of pred and X_test must match\"\n",
    "\n",
    "    # Compute the number of differing bits\n",
    "    incorrect_bits = np.sum(pred != X_test)\n",
    "\n",
    "    # Total number of bits\n",
    "    total_bits = np.prod(X_test.shape)\n",
    "\n",
    "    # Hamming loss is the fraction of incorrect bits\n",
    "    return incorrect_bits / total_bits"
   ],
   "id": "6188dfae59ac0259",
   "outputs": [],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T10:51:22.120487Z",
     "start_time": "2025-04-16T10:51:22.116575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(tm, X):\n",
    "    pred = tm.predict(X)\n",
    "\n",
    "    loss = [hamming_loss(X[i], pred[i]) for i in range(len(X))]\n",
    "\n",
    "    return np.mean(loss), np.median(loss), np.max(loss), np.min(loss)"
   ],
   "id": "8b4cf0985093a6bd",
   "outputs": [],
   "execution_count": 129
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T10:51:22.132058Z",
     "start_time": "2025-04-16T10:51:22.127076Z"
    }
   },
   "source": [
    "def train(args):\n",
    "    tm = TMAutoEncoder(\n",
    "        number_of_clauses=args[\"num_clauses\"],\n",
    "        T=args[\"T\"],\n",
    "        s=args[\"s\"],\n",
    "        output_active=args[\"output_active\"],\n",
    "        max_included_literals=args[\"max_included_literals\"],\n",
    "        accumulation=args[\"accumulation\"],\n",
    "        feature_negation=args[\"feature_negation\"],\n",
    "        platform=args[\"platform\"],\n",
    "        output_balancing=args[\"output_balancing\"],\n",
    "    )\n",
    "\n",
    "    print(f\"Starting training for {args['epochs']} epochs\")\n",
    "\n",
    "    for e in range(args[\"epochs\"]):\n",
    "        tm.fit(X_train, number_of_examples=args[\"number_of_examples\"])\n",
    "\n",
    "        lmean, lmed, lmax, lmin = test(tm, X_test)\n",
    "        print(f\"Epoch: {e + 1} Mean loss: {lmean:4f}, Median loss: {lmed:4f}, Max loss: {lmax:4f}, Min loss: {lmin:4f}\")\n",
    "\n",
    "        save_model(tm, f\"latest_{e}.pkl\")\n"
   ],
   "outputs": [],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T10:51:22.140468Z",
     "start_time": "2025-04-16T10:51:22.137619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "number_of_features = X_train.shape[1]\n",
    "output_active = np.arange(number_of_features, dtype=np.uint32)\n",
    "\n",
    "number_of_clauses = 2000\n",
    "\n",
    "print(f\"Number of features {number_of_features}\")\n",
    "print(f\"Number of clauses {number_of_clauses}\")"
   ],
   "id": "e39fbba456bd25dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features 2000\n",
      "Number of clauses 2000\n"
     ]
    }
   ],
   "execution_count": 131
  },
  {
   "cell_type": "code",
   "id": "d4dc7e4da5c88e88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T17:10:10.233487Z",
     "start_time": "2025-04-16T10:51:22.217956Z"
    }
   },
   "source": [
    "args: dict = {\n",
    "    \"number_of_examples\": 1000,\n",
    "    \"output_active\": output_active,\n",
    "    \"accumulation\": 1,\n",
    "    \"num_clauses\": number_of_clauses,\n",
    "    \"T\": int(number_of_clauses * 0.75),\n",
    "    \"s\": 15.0,\n",
    "    \"epochs\": 25,\n",
    "    \"platform\": \"CPU\",\n",
    "    \"output_balancing\": 0,\n",
    "    \"max_included_literals\": 4000,\n",
    "    \"feature_negation\": True,\n",
    "}\n",
    "\n",
    "result = train(args)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 25 epochs\n",
      "Epoch: 1 Mean loss: 0.530718, Median loss: 0.539500, Max loss: 0.577500, Min loss: 0.471500\n",
      "Epoch: 2 Mean loss: 0.504766, Median loss: 0.499500, Max loss: 0.570500, Min loss: 0.461000\n",
      "Epoch: 3 Mean loss: 0.496124, Median loss: 0.496500, Max loss: 0.565000, Min loss: 0.458500\n",
      "Epoch: 4 Mean loss: 0.496080, Median loss: 0.496000, Max loss: 0.527500, Min loss: 0.457000\n",
      "Epoch: 5 Mean loss: 0.493961, Median loss: 0.494000, Max loss: 0.529000, Min loss: 0.460000\n",
      "Epoch: 6 Mean loss: 0.494947, Median loss: 0.495000, Max loss: 0.527000, Min loss: 0.465000\n",
      "Epoch: 7 Mean loss: 0.496796, Median loss: 0.496500, Max loss: 0.530500, Min loss: 0.455500\n",
      "Epoch: 8 Mean loss: 0.492488, Median loss: 0.492000, Max loss: 0.527500, Min loss: 0.455000\n",
      "Epoch: 9 Mean loss: 0.492953, Median loss: 0.493500, Max loss: 0.523000, Min loss: 0.460000\n",
      "Epoch: 10 Mean loss: 0.492720, Median loss: 0.492000, Max loss: 0.530000, Min loss: 0.451000\n",
      "Epoch: 11 Mean loss: 0.493554, Median loss: 0.493500, Max loss: 0.525000, Min loss: 0.453500\n",
      "Epoch: 12 Mean loss: 0.495221, Median loss: 0.495000, Max loss: 0.530500, Min loss: 0.456000\n",
      "Epoch: 13 Mean loss: 0.491501, Median loss: 0.491000, Max loss: 0.530500, Min loss: 0.452500\n",
      "Epoch: 14 Mean loss: 0.492217, Median loss: 0.492000, Max loss: 0.523000, Min loss: 0.454500\n",
      "Epoch: 15 Mean loss: 0.494074, Median loss: 0.494500, Max loss: 0.523000, Min loss: 0.446000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[132], line 15\u001B[0m\n\u001B[1;32m      1\u001B[0m args: \u001B[38;5;28mdict\u001B[39m \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumber_of_examples\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1000\u001B[39m,\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_active\u001B[39m\u001B[38;5;124m\"\u001B[39m: output_active,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature_negation\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     13\u001B[0m }\n\u001B[0;32m---> 15\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[130], line 17\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting training for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00margs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m epochs\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(args[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m\"\u001B[39m]):\n\u001B[0;32m---> 17\u001B[0m     \u001B[43mtm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumber_of_examples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnumber_of_examples\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m     lmean, lmed, lmax, lmin \u001B[38;5;241m=\u001B[39m test(tm, X_test)\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Mean loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlmean\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Median loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlmed\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Max loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlmax\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Min loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlmin\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/GitHub/uia-master-thesis/.venv/lib/python3.12/site-packages/tmu/models/autoencoder/autoencoder.py:292\u001B[0m, in \u001B[0;36mTMAutoEncoder.fit\u001B[0;34m(self, X, number_of_examples, shuffle, *kwargs)\u001B[0m\n\u001B[1;32m    288\u001B[0m     literal_active[ta_chunk_negated] \u001B[38;5;241m&\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m~\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m<<\u001B[39m chunk_pos_negated)\n\u001B[1;32m    290\u001B[0m literal_active[ta_chunk] \u001B[38;5;241m&\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m~\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m<<\u001B[39m chunk_pos)\n\u001B[0;32m--> 292\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mYu\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mXu\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mupdate_clause\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mclause_active\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mliteral_active\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_negation:\n\u001B[1;32m    295\u001B[0m     literal_active[ta_chunk_negated] \u001B[38;5;241m=\u001B[39m copy_literal_active_ta_chunk_negated\n",
      "File \u001B[0;32m~/Documents/GitHub/uia-master-thesis/.venv/lib/python3.12/site-packages/tmu/models/autoencoder/autoencoder.py:122\u001B[0m, in \u001B[0;36mTMAutoEncoder.update\u001B[0;34m(self, target_output, Y, encoded_X, clause_active, literal_active)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate\u001B[39m(\n\u001B[1;32m    114\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    115\u001B[0m         target_output,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    119\u001B[0m         literal_active\n\u001B[1;32m    120\u001B[0m ):\n\u001B[1;32m    121\u001B[0m     all_literal_active \u001B[38;5;241m=\u001B[39m (np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclause_bank\u001B[38;5;241m.\u001B[39mnumber_of_ta_chunks, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39muint32) \u001B[38;5;241m|\u001B[39m \u001B[38;5;241m~\u001B[39m\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39muint32)\n\u001B[0;32m--> 122\u001B[0m     clause_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclause_bank\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcalculate_clause_outputs_update\u001B[49m\u001B[43m(\u001B[49m\u001B[43mall_literal_active\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoded_X\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    124\u001B[0m     class_sum \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(clause_active \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight_banks[target_output]\u001B[38;5;241m.\u001B[39mget_weights(), clause_outputs)\u001B[38;5;241m.\u001B[39mastype(\n\u001B[1;32m    125\u001B[0m         np\u001B[38;5;241m.\u001B[39mint32)\n\u001B[1;32m    126\u001B[0m     class_sum \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mclip(class_sum, \u001B[38;5;241m-\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mT, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mT)\n",
      "File \u001B[0;32m~/Documents/GitHub/uia-master-thesis/.venv/lib/python3.12/site-packages/tmu/clause_bank/clause_bank.py:206\u001B[0m, in \u001B[0;36mClauseBank.calculate_clause_outputs_update\u001B[0;34m(self, literal_active, encoded_X, e)\u001B[0m\n\u001B[1;32m    203\u001B[0m xi_p \u001B[38;5;241m=\u001B[39m ffi\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munsigned int *\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoded_X[e, :]\u001B[38;5;241m.\u001B[39mctypes\u001B[38;5;241m.\u001B[39mdata)\n\u001B[1;32m    204\u001B[0m la_p \u001B[38;5;241m=\u001B[39m ffi\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munsigned int *\u001B[39m\u001B[38;5;124m\"\u001B[39m, literal_active\u001B[38;5;241m.\u001B[39mctypes\u001B[38;5;241m.\u001B[39mdata)\n\u001B[0;32m--> 206\u001B[0m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcb_calculate_clause_outputs_update\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    207\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mptr_ta_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    208\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumber_of_clauses\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    209\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumber_of_literals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    210\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumber_of_state_bits_ta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    211\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumber_of_patches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    212\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mco_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    213\u001B[0m \u001B[43m    \u001B[49m\u001B[43mla_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    214\u001B[0m \u001B[43m    \u001B[49m\u001B[43mxi_p\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclause_output\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 132
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
