{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T16:46:12.120410Z",
     "start_time": "2025-04-23T16:46:11.747579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import all required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ],
   "id": "68f65dd8847de2f6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T16:46:12.125713Z",
     "start_time": "2025-04-23T16:46:12.123746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(42)"
   ],
   "id": "c39f852fe0a113d1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T16:46:12.225582Z",
     "start_time": "2025-04-23T16:46:12.219556Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def binarize_dataset_with_hdc(dataset, D=2000, Q=8, n_gram=3, show_details=False):\n",
    "    \"\"\"\n",
    "    Binarize a multi-feature dataset using Hyperdimensional Computing (HDC).\n",
    "\n",
    "    Parameters:\n",
    "        dataset (np.ndarray): Dataset with shape [num_samples, num_features]\n",
    "        D (int): Dimension of hypervectors\n",
    "        Q (int): Number of quantization levels\n",
    "        n_gram (int): Size of N-gram window for sequence encoding\n",
    "        show_details (bool): Whether to print detailed output\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of binary hypervectors, one for each sample\n",
    "    \"\"\"\n",
    "\n",
    "    num_samples, num_features = dataset.shape\n",
    "\n",
    "    if show_details:\n",
    "        print(f\"Dataset shape: {dataset.shape}\")\n",
    "        print(f\"Number of samples: {num_samples}\")\n",
    "        print(f\"Number of features: {num_features}\")\n",
    "\n",
    "    # HDC operations\n",
    "    def generate_random_hypervector(D):\n",
    "        \"\"\"Generate a random binary hypervector of dimension D\"\"\"\n",
    "        return np.random.randint(0, 2, D, dtype=np.uint8)\n",
    "\n",
    "    def bind(hv1, hv2):\n",
    "        \"\"\"Binding operation (XOR)\"\"\"\n",
    "        return np.logical_xor(hv1, hv2).astype(np.uint8)\n",
    "\n",
    "    def bundle(hvs):\n",
    "        \"\"\"Bundling operation (majority vote)\"\"\"\n",
    "        stacked = np.vstack(hvs)\n",
    "        counts = np.sum(stacked, axis=0)\n",
    "\n",
    "        # Majority voting\n",
    "        threshold = len(hvs) / 2\n",
    "        return (counts > threshold).astype(np.uint8)\n",
    "\n",
    "    # Find global min and max for each feature\n",
    "    min_values = np.min(dataset, axis=0)\n",
    "    max_values = np.max(dataset, axis=0)\n",
    "\n",
    "    # Generate feature ID vectors\n",
    "    feature_id_vectors = [generate_random_hypervector(D) for _ in range(num_features)]\n",
    "\n",
    "    # Generate interval vectors for each feature\n",
    "    # We'll use the same quantization levels for all features, but different random vectors\n",
    "    all_interval_vectors = []\n",
    "    for feature_idx in range(num_features):\n",
    "        feature_interval_vectors = [generate_random_hypervector(D) for _ in range(Q)]\n",
    "        all_interval_vectors.append(feature_interval_vectors)\n",
    "\n",
    "    # Generate position vectors for N-gram encoding\n",
    "    position_vectors = [generate_random_hypervector(D) for _ in range(n_gram)]\n",
    "\n",
    "    # Generate gram vectors\n",
    "    gram_vectors = [generate_random_hypervector(D) for _ in range(n_gram)]\n",
    "\n",
    "    # Binarize the dataset\n",
    "    binarized_samples = []\n",
    "\n",
    "    for sample_idx in range(num_samples):\n",
    "        # Get the feature vector for this sample\n",
    "        sample = dataset[sample_idx]\n",
    "\n",
    "        if show_details and sample_idx == 0:\n",
    "            print(f\"\\nProcessing sample 0: {sample}\")\n",
    "\n",
    "        # Encode each feature value into a hypervector\n",
    "        feature_hvs = []\n",
    "\n",
    "        for feature_idx in range(num_features):\n",
    "            value = sample[feature_idx]\n",
    "            min_val = min_values[feature_idx]\n",
    "            max_val = max_values[feature_idx]\n",
    "\n",
    "            # Skip features with no variation\n",
    "            if min_val == max_val:\n",
    "                continue\n",
    "\n",
    "            # Quantize the value\n",
    "            step = (max_val - min_val) / Q\n",
    "            bucket_idx = min(Q - 1, max(0, int((value - min_val) / step)))\n",
    "            value_hv = all_interval_vectors[feature_idx][bucket_idx]\n",
    "\n",
    "            # Bind the value HV with the feature ID to create a unique representation\n",
    "            feature_hv = bind(value_hv, feature_id_vectors[feature_idx])\n",
    "            feature_hvs.append(feature_hv)\n",
    "\n",
    "            if show_details and sample_idx == 0 and feature_idx < 3:\n",
    "                # Only show the first 3 features for the first sample\n",
    "                print(f\"  Feature {feature_idx} value: {value:.4f}, bucket: {bucket_idx}\")\n",
    "\n",
    "        # APPROACH 1: Bundle all feature hypervectors\n",
    "        # This is simpler but doesn't capture inter-feature relationships\n",
    "        if num_features <= n_gram:\n",
    "            # If we have fewer features than n_gram size, just bundle them\n",
    "            sample_hv = bundle(feature_hvs)\n",
    "        else:\n",
    "            # Apply N-gram encoding to capture relationships between adjacent features\n",
    "            ngram_hvs = []\n",
    "\n",
    "            for i in range(num_features - n_gram + 1):\n",
    "                gram_elements = []\n",
    "\n",
    "                for j in range(n_gram):\n",
    "                    feature_idx = i + j\n",
    "                    if feature_idx < len(feature_hvs):  # Check if within bounds\n",
    "                        hv = feature_hvs[feature_idx]\n",
    "\n",
    "                        # Bind with position vector\n",
    "                        pos_bound_hv = bind(hv, position_vectors[j])\n",
    "\n",
    "                        # Bind with gram vector\n",
    "                        bound_hv = bind(pos_bound_hv, gram_vectors[j])\n",
    "                        gram_elements.append(bound_hv)\n",
    "\n",
    "                # Bundle this N-gram\n",
    "                if gram_elements:  # Check if we have elements to bundle\n",
    "                    ngram_hv = bundle(gram_elements)\n",
    "                    ngram_hvs.append(ngram_hv)\n",
    "\n",
    "            # Bundle all N-grams to get final representation\n",
    "            if ngram_hvs:  # Check if we have N-grams to bundle\n",
    "                sample_hv = bundle(ngram_hvs)\n",
    "            else:\n",
    "                # Fallback: just bundle all feature vectors\n",
    "                sample_hv = bundle(feature_hvs)\n",
    "\n",
    "        binarized_samples.append(sample_hv)\n",
    "\n",
    "        if show_details and sample_idx == 0:\n",
    "            # Count ones in the first sample's binary vector\n",
    "            ones_count = np.sum(sample_hv)\n",
    "            print(\n",
    "                f\"  Sample 0 binary vector: {len(sample_hv)} bits, {ones_count} ones ({ones_count / len(sample_hv):.4f})\")\n",
    "\n",
    "    # Stack all sample hypervectors into a single array\n",
    "    binarized_dataset = np.vstack(binarized_samples)\n",
    "\n",
    "    if show_details:\n",
    "        print(f\"\\nBinarized dataset shape: {binarized_dataset.shape}\")\n",
    "\n",
    "    return binarized_dataset"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T16:46:12.233014Z",
     "start_time": "2025-04-23T16:46:12.230142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wind_farm = \"B\"\n",
    "train_datasets = [34, 7]\n",
    "test_datasets = [34, 7, 53, 27, 19, 77, 83, 52, 21, 2, 23, 87, 74, 86, 82]\n",
    "\n",
    "# [34, 7, 53, 27, 19, 77, 83, 52, 21, 2, 23, 87, 74, 86, 82]"
   ],
   "id": "feaf8573a94e6196",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T16:46:12.243008Z",
     "start_time": "2025-04-23T16:46:12.239984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create folders data_test and data_train if they do not exist\n",
    "os.makedirs(\"data_test\", exist_ok=True)\n",
    "os.makedirs(\"data_train\", exist_ok=True)"
   ],
   "id": "f44feaaab5d59a69",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T16:46:12.254161Z",
     "start_time": "2025-04-23T16:46:12.250963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "exclude_columns = [\"time_stamp\", \"asset_id\", \"id\"]\n",
    "\n",
    "\n",
    "def load_df_and_annotate_anomalies(farm, event_id):\n",
    "    path = f\"../../../data/care_to_compare/Wind Farm {farm}/datasets/{event_id}.csv\"\n",
    "    df = pd.read_csv(path, delimiter=';')\n",
    "\n",
    "    event_info = pd.read_csv(f\"../../../data/care_to_compare/Wind Farm {farm}/event_info.csv\", delimiter=';')\n",
    "\n",
    "    # Find the row where event_id = event_id\n",
    "    metadata = event_info[event_info['event_id'] == event_id]\n",
    "\n",
    "    event_label = metadata[\"event_label\"].values[0]\n",
    "    event_start_id = metadata[\"event_start_id\"].values[0]\n",
    "    event_end_id = metadata[\"event_end_id\"].values[0]\n",
    "\n",
    "    label_value = 1 if event_label == \"anomaly\" else 0\n",
    "\n",
    "    # All rows where the column \"id\" is between event_start_id and event_end_id\n",
    "    df['label'] = 0\n",
    "    df.loc[(df['id'] >= event_start_id) & (df['id'] <= event_end_id), 'label'] = label_value\n",
    "\n",
    "    # Include all columns except for the ones in exclude_columns\n",
    "    df = df[[col for col in df.columns if col not in exclude_columns]]\n",
    "\n",
    "    # Replace inf values with NaN and drop rows with NaN values\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df"
   ],
   "id": "767812d8e26efd53",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T16:46:12.261255Z",
     "start_time": "2025-04-23T16:46:12.257707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def binarize_dataset_for_training(farm, event_id, output_path):\n",
    "    # Load original dataset from file\n",
    "    df = load_df_and_annotate_anomalies(farm, event_id)\n",
    "    df = df[df['train_test'] == 'train']\n",
    "\n",
    "    # Take only 1000 rows\n",
    "    #df = df[:1000]\n",
    "\n",
    "    df = df[df['status_type_id'].isin([0, 2])]\n",
    "\n",
    "    # Split into data and labels\n",
    "    X_values = df.drop(columns=['label', 'train_test', 'status_type_id'])\n",
    "    X_values = X_values.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Encode the sequence\n",
    "    final_binary_vector = binarize_dataset_with_hdc(X_values.values, show_details=True)\n",
    "\n",
    "    # Output to file using np\n",
    "    np.savetxt(f\"{output_path}/X_{farm}_{event_id}.txt\", final_binary_vector, fmt='%d')\n",
    "\n",
    "    label_df = pd.DataFrame({\n",
    "        'label': df['label'].values,\n",
    "        'status_type_id': df['status_type_id'].values,\n",
    "        'train_test': df['train_test'].values\n",
    "    })\n",
    "\n",
    "    label_df.to_csv(f\"{output_path}/y_{farm}_{event_id}.csv\", index=False)\n",
    "\n",
    "    print(f\"Done with {event_id}\")\n",
    "\n",
    "    # Calculate statistics\n",
    "    number_of_0s = np.sum(final_binary_vector == 0)\n",
    "    number_of_1s = np.sum(final_binary_vector == 1)\n",
    "\n",
    "    total_bits = len(final_binary_vector)\n",
    "\n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"Total bits: {total_bits}\")\n",
    "    print(f\"Number of 1s: {number_of_1s}\")\n",
    "    print(f\"Number of 0s: {number_of_0s}\")\n",
    "\n",
    "\n",
    "def binarize_dataset_for_testing(farm, event_id, output_path):\n",
    "    # Load original dataset from file\n",
    "    df = load_df_and_annotate_anomalies(farm, event_id)\n",
    "\n",
    "    # Only take the data that is in the prediction set\n",
    "    df = df[df['train_test'] == 'prediction']\n",
    "\n",
    "    # Split into data and labels\n",
    "    X_values = df.drop(columns=['label', 'train_test', 'status_type_id'])\n",
    "    X_values = X_values.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Encode the sequence\n",
    "    final_binary_vector = binarize_dataset_with_hdc(X_values.values)\n",
    "\n",
    "    # Output to file using np\n",
    "    np.savetxt(f\"{output_path}/X_{farm}_{event_id}.txt\", final_binary_vector, fmt='%d')\n",
    "\n",
    "    label_df = pd.DataFrame({\n",
    "        'label': df['label'].values,\n",
    "        'status_type_id': df['status_type_id'].values,\n",
    "        'train_test': df['train_test'].values\n",
    "    })\n",
    "\n",
    "    label_df.to_csv(f\"{output_path}/y_{farm}_{event_id}.csv\", index=False)\n",
    "\n",
    "    # Calculate statistics\n",
    "    total_bits = len(final_binary_vector)\n",
    "\n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"Total rows: {total_bits}\")\n",
    "    print(f\"Total columns: {len(final_binary_vector[0])}\")\n"
   ],
   "id": "6c104fdf24752f99",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T16:46:19.523278Z",
     "start_time": "2025-04-23T16:46:12.268914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset in train_datasets:\n",
    "    binarize_dataset_for_training(wind_farm, dataset, \"./data_train\")"
   ],
   "id": "89841ee82e5285d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (43971, 252)\n",
      "Number of samples: 43971\n",
      "Number of features: 252\n",
      "\n",
      "Processing sample 0: [ 9.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  2.89000000e+02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.82400000e+01  6.56300000e+01  1.45900000e+01  3.39650000e+02\n",
      " -2.97530000e+02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  6.09000000e+00  9.64000000e+00  1.39000000e+00  1.91000000e+00\n",
      "  1.23900000e+01  1.24100000e+01  0.00000000e+00  1.23800000e+01\n",
      "  1.74700000e+01  1.75000000e+01  4.00000000e-02  1.74000000e+01\n",
      "  2.20260000e+02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  2.00000000e-02  3.00000000e-02  1.00000000e-02  0.00000000e+00\n",
      " -9.55000000e-03 -7.76935484e-03  8.08064516e-04 -1.17693548e-02\n",
      "  1.89285000e+04  2.27147000e+04  1.29986000e+03  1.57026000e+04\n",
      "  1.92353000e+04  1.92454000e+04  3.09000000e+00  1.92293000e+04\n",
      "  1.92584000e+04  1.92673000e+04  3.25000000e+00  1.92501000e+04\n",
      "  1.92519000e+04  1.92619000e+04  3.83000000e+00  1.92431000e+04\n",
      "  3.33700000e+01  4.23200000e+01  3.06000000e+00  2.63600000e+01\n",
      "  3.33200000e+01  4.23200000e+01  3.08000000e+00  2.63100000e+01\n",
      "  3.34000000e+01  4.24100000e+01  3.03000000e+00  2.64500000e+01\n",
      "  9.62240000e+02  1.02527000e+03  2.53100000e+01  9.14730000e+02\n",
      "  9.64420000e+02  1.02763000e+03  2.53500000e+01  9.17190000e+02\n",
      "  1.92900000e+01  2.64500000e+01  2.41000000e+00  1.79100000e+01\n",
      "  2.98200000e+01  3.00000000e+01  3.30000000e-01  2.90000000e+01\n",
      "  4.98900000e+01  4.98900000e+01  0.00000000e+00  4.98800000e+01\n",
      "  3.33395000e+04  3.33549000e+04  5.60000000e+00  3.33290000e+04\n",
      "  8.33000000e+00  8.88000000e+00  2.20000000e-01  7.92000000e+00\n",
      "  3.08954839e-01  2.43992000e+03  1.76320000e+02  1.50968000e+03\n",
      "  3.33600000e+01  4.23500000e+01  3.05000000e+00  2.63800000e+01\n",
      "  1.96000000e+00  3.25200000e+01  4.09000000e+00  0.00000000e+00\n",
      "  1.02000000e+00  3.26500000e+01  4.22000000e+00  0.00000000e+00\n",
      "  1.93000000e+00  3.19300000e+01  3.93000000e+00  9.80000000e-01\n",
      "  4.63600000e+01  4.67000000e+01  1.20000000e-01  4.60500000e+01\n",
      "  4.97800000e+01  5.00000000e+01  5.00000000e-02  4.96500000e+01\n",
      "  4.09200000e+01  4.10000000e+01  3.00000000e-02  4.09000000e+01\n",
      "  6.48300000e+01  6.63000000e+01  1.54000000e+00  6.16000000e+01\n",
      "  6.55900000e+01  6.73000000e+01  1.88000000e+00  6.16000000e+01\n",
      "  5.75800000e+01  5.88000000e+01  1.18000000e+00  5.49000000e+01\n",
      "  5.82000000e+01  5.97000000e+01  1.32000000e+00  5.54000000e+01\n",
      "  4.82800000e+01  5.66000000e+01  8.12000000e+00  3.70000000e+01\n",
      "  5.85300000e+01  6.04000000e+01  2.00000000e+00  5.39000000e+01\n",
      "  1.10200000e+02  1.10400000e+02  4.00000000e-02  1.10100000e+02\n",
      "  5.16200000e+01  5.19500000e+01  1.40000000e-01  5.13000000e+01\n",
      "  6.74500000e+01  6.78500000e+01  1.30000000e-01  6.71000000e+01\n",
      "  5.27300000e+01  5.33500000e+01  2.40000000e-01  5.21000000e+01\n",
      "  7.28400000e+01  7.35000000e+01  2.40000000e-01  7.22500000e+01\n",
      "  5.04800000e+01  5.09000000e+01  1.20000000e-01  5.01000000e+01\n",
      "  6.25500000e+01  6.26500000e+01  3.00000000e-02  6.24500000e+01\n",
      "  6.23400000e+01  6.34500000e+01  4.30000000e-01  6.13500000e+01\n",
      "  2.60600000e+01  2.70000000e+01  1.80000000e-01  2.60000000e+01\n",
      "  2.72000000e+01  2.80000000e+01  2.80000000e-01  2.70000000e+01\n",
      "  2.87200000e+01  2.90000000e+01  3.00000000e-01  2.80000000e+01\n",
      "  4.52200000e+01  4.53800000e+01  6.00000000e-02  4.51000000e+01\n",
      "  5.41800000e+01  5.42900000e+01  3.00000000e-02  5.40800000e+01\n",
      "  2.75600000e+01  2.78000000e+01  9.00000000e-02  2.73500000e+01\n",
      "  2.80000000e-01  6.20000000e-01  8.00000000e-02  1.10000000e-01\n",
      "  4.23000000e+00  1.49400000e+01  2.64000000e+00  2.30000000e-01\n",
      "  5.17000000e+00  1.47700000e+01  2.91000000e+00  5.00000000e-02\n",
      "  1.92523000e+04  2.30151000e+04  1.31325000e+03  1.60084000e+04\n",
      "  3.09201613e-01  3.70580645e-01  2.43064516e-02  2.64698387e-01\n",
      "  8.24000000e+00  1.09500000e+01  1.11000000e+00  4.20000000e+00\n",
      "  7.45000000e+00  1.14200000e+01  1.55000000e+00  2.77000000e+00\n",
      "  7.84000000e+00  1.06400000e+01  1.12000000e+00  4.14000000e+00\n",
      "  3.08801613e-01  3.93414516e-01  2.84516129e-02  2.43291935e-01]\n",
      "  Feature 0 value: 9.0000, bucket: 0\n",
      "  Sample 0 binary vector: 2000 bits, 894 ones (0.4470)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m train_datasets:\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mbinarize_dataset_for_training\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwind_farm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./data_train\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[7], line 16\u001B[0m, in \u001B[0;36mbinarize_dataset_for_training\u001B[0;34m(farm, event_id, output_path)\u001B[0m\n\u001B[1;32m     13\u001B[0m X_values \u001B[38;5;241m=\u001B[39m X_values\u001B[38;5;241m.\u001B[39mapply(pd\u001B[38;5;241m.\u001B[39mto_numeric, errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcoerce\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Encode the sequence\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m final_binary_vector \u001B[38;5;241m=\u001B[39m \u001B[43mbinarize_dataset_with_hdc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_values\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshow_details\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Output to file using np\u001B[39;00m\n\u001B[1;32m     19\u001B[0m np\u001B[38;5;241m.\u001B[39msavetxt(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutput_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/X_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfarm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mevent_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m, final_binary_vector, fmt\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[3], line 125\u001B[0m, in \u001B[0;36mbinarize_dataset_with_hdc\u001B[0;34m(dataset, D, Q, n_gram, show_details)\u001B[0m\n\u001B[1;32m    123\u001B[0m     \u001B[38;5;66;03m# Bundle this N-gram\u001B[39;00m\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gram_elements:  \u001B[38;5;66;03m# Check if we have elements to bundle\u001B[39;00m\n\u001B[0;32m--> 125\u001B[0m         ngram_hv \u001B[38;5;241m=\u001B[39m \u001B[43mbundle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgram_elements\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    126\u001B[0m         ngram_hvs\u001B[38;5;241m.\u001B[39mappend(ngram_hv)\n\u001B[1;32m    128\u001B[0m \u001B[38;5;66;03m# Bundle all N-grams to get final representation\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[3], line 38\u001B[0m, in \u001B[0;36mbinarize_dataset_with_hdc.<locals>.bundle\u001B[0;34m(hvs)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Bundling operation (majority vote)\"\"\"\u001B[39;00m\n\u001B[1;32m     37\u001B[0m stacked \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mvstack(hvs)\n\u001B[0;32m---> 38\u001B[0m counts \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstacked\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Majority voting\u001B[39;00m\n\u001B[1;32m     41\u001B[0m threshold \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(hvs) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m\n",
      "File \u001B[0;32m~/Documents/GitHub/uia-master-thesis/.venv/lib/python3.12/site-packages/numpy/core/fromnumeric.py:2313\u001B[0m, in \u001B[0;36msum\u001B[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m   2310\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[1;32m   2311\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\u001B[0;32m-> 2313\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapreduction\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2314\u001B[0m \u001B[43m                      \u001B[49m\u001B[43minitial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/uia-master-thesis/.venv/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88\u001B[0m, in \u001B[0;36m_wrapreduction\u001B[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001B[0m\n\u001B[1;32m     85\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     86\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m reduction(axis\u001B[38;5;241m=\u001B[39maxis, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpasskwargs)\n\u001B[0;32m---> 88\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mufunc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpasskwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T16:46:19.530949Z",
     "start_time": "2025-04-16T05:41:31.276735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset in test_datasets:\n",
    "    binarize_dataset_for_testing(wind_farm, dataset, \"./data_test\")"
   ],
   "id": "a4f5f329094e459e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics:\n",
      "Total rows: 4033\n",
      "Total columns: 2000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 5328\n",
      "Total columns: 2000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 6048\n",
      "Total columns: 2000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 9937\n",
      "Total columns: 2000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 3745\n",
      "Total columns: 2000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 9217\n",
      "Total columns: 2000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 14113\n",
      "Total columns: 2000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2737\n",
      "Total columns: 2000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1297\n",
      "Total columns: 2000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2215\n",
      "Total columns: 2000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1983\n",
      "Total columns: 2000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 3025\n",
      "Total columns: 2000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 3073\n",
      "Total columns: 2000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2927\n",
      "Total columns: 2000\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2449\n",
      "Total columns: 2000\n"
     ]
    }
   ],
   "execution_count": 130
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
