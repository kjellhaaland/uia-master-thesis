{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T17:07:03.756144Z",
     "start_time": "2025-04-23T17:07:03.752754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import all required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ],
   "id": "68f65dd8847de2f6",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T17:07:03.764167Z",
     "start_time": "2025-04-23T17:07:03.761898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(42)"
   ],
   "id": "c39f852fe0a113d1",
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-23T17:07:03.785911Z",
     "start_time": "2025-04-23T17:07:03.775319Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def binarize_dataset_with_hdc(dataset, D=7600, Q=8, n_gram=3, show_details=False):\n",
    "    \"\"\"\n",
    "    Binarize a multi-feature dataset using Hyperdimensional Computing (HDC).\n",
    "\n",
    "    Parameters:\n",
    "        dataset (np.ndarray): Dataset with shape [num_samples, num_features]\n",
    "        D (int): Dimension of hypervectors\n",
    "        Q (int): Number of quantization levels\n",
    "        n_gram (int): Size of N-gram window for sequence encoding\n",
    "        show_details (bool): Whether to print detailed output\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of binary hypervectors, one for each sample\n",
    "    \"\"\"\n",
    "\n",
    "    num_samples, num_features = dataset.shape\n",
    "\n",
    "    if show_details:\n",
    "        print(f\"Dataset shape: {dataset.shape}\")\n",
    "        print(f\"Number of samples: {num_samples}\")\n",
    "        print(f\"Number of features: {num_features}\")\n",
    "\n",
    "    # HDC operations\n",
    "    def generate_random_hypervector(D):\n",
    "        \"\"\"Generate a random binary hypervector of dimension D\"\"\"\n",
    "        return np.random.randint(0, 2, D, dtype=np.uint8)\n",
    "\n",
    "    def bind(hv1, hv2):\n",
    "        \"\"\"Binding operation (XOR)\"\"\"\n",
    "        return np.logical_xor(hv1, hv2).astype(np.uint8)\n",
    "\n",
    "    def bundle(hvs):\n",
    "        \"\"\"Bundling operation (majority vote)\"\"\"\n",
    "        stacked = np.vstack(hvs)\n",
    "        counts = np.sum(stacked, axis=0)\n",
    "\n",
    "        # Majority voting\n",
    "        threshold = len(hvs) / 2\n",
    "        return (counts > threshold).astype(np.uint8)\n",
    "\n",
    "    # Find global min and max for each feature\n",
    "    min_values = np.min(dataset, axis=0)\n",
    "    max_values = np.max(dataset, axis=0)\n",
    "\n",
    "    # Generate feature ID vectors\n",
    "    feature_id_vectors = [generate_random_hypervector(D) for _ in range(num_features)]\n",
    "\n",
    "    # Generate interval vectors for each feature\n",
    "    # We'll use the same quantization levels for all features, but different random vectors\n",
    "    all_interval_vectors = []\n",
    "    for feature_idx in range(num_features):\n",
    "        feature_interval_vectors = [generate_random_hypervector(D) for _ in range(Q)]\n",
    "        all_interval_vectors.append(feature_interval_vectors)\n",
    "\n",
    "    # Generate position vectors for N-gram encoding\n",
    "    position_vectors = [generate_random_hypervector(D) for _ in range(n_gram)]\n",
    "\n",
    "    # Generate gram vectors\n",
    "    gram_vectors = [generate_random_hypervector(D) for _ in range(n_gram)]\n",
    "\n",
    "    # Binarize the dataset\n",
    "    binarized_samples = []\n",
    "\n",
    "    for sample_idx in range(num_samples):\n",
    "        # Get the feature vector for this sample\n",
    "        sample = dataset[sample_idx]\n",
    "\n",
    "        if show_details and sample_idx == 0:\n",
    "            print(f\"\\nProcessing sample 0: {sample}\")\n",
    "\n",
    "        # Encode each feature value into a hypervector\n",
    "        feature_hvs = []\n",
    "\n",
    "        for feature_idx in range(num_features):\n",
    "            value = sample[feature_idx]\n",
    "            min_val = min_values[feature_idx]\n",
    "            max_val = max_values[feature_idx]\n",
    "\n",
    "            # Skip features with no variation\n",
    "            if min_val == max_val:\n",
    "                continue\n",
    "\n",
    "            # Quantize the value\n",
    "            step = (max_val - min_val) / Q\n",
    "            bucket_idx = min(Q - 1, max(0, int((value - min_val) / step)))\n",
    "            value_hv = all_interval_vectors[feature_idx][bucket_idx]\n",
    "\n",
    "            # Bind the value HV with the feature ID to create a unique representation\n",
    "            feature_hv = bind(value_hv, feature_id_vectors[feature_idx])\n",
    "            feature_hvs.append(feature_hv)\n",
    "\n",
    "            if show_details and sample_idx == 0 and feature_idx < 3:\n",
    "                # Only show the first 3 features for the first sample\n",
    "                print(f\"  Feature {feature_idx} value: {value:.4f}, bucket: {bucket_idx}\")\n",
    "\n",
    "        # APPROACH 1: Bundle all feature hypervectors\n",
    "        # This is simpler but doesn't capture inter-feature relationships\n",
    "        if num_features <= n_gram:\n",
    "            # If we have fewer features than n_gram size, just bundle them\n",
    "            sample_hv = bundle(feature_hvs)\n",
    "        else:\n",
    "            # Apply N-gram encoding to capture relationships between adjacent features\n",
    "            ngram_hvs = []\n",
    "\n",
    "            for i in range(num_features - n_gram + 1):\n",
    "                gram_elements = []\n",
    "\n",
    "                for j in range(n_gram):\n",
    "                    feature_idx = i + j\n",
    "                    if feature_idx < len(feature_hvs):  # Check if within bounds\n",
    "                        hv = feature_hvs[feature_idx]\n",
    "\n",
    "                        # Bind with position vector\n",
    "                        pos_bound_hv = bind(hv, position_vectors[j])\n",
    "\n",
    "                        # Bind with gram vector\n",
    "                        bound_hv = bind(pos_bound_hv, gram_vectors[j])\n",
    "                        gram_elements.append(bound_hv)\n",
    "\n",
    "                # Bundle this N-gram\n",
    "                if gram_elements:  # Check if we have elements to bundle\n",
    "                    ngram_hv = bundle(gram_elements)\n",
    "                    ngram_hvs.append(ngram_hv)\n",
    "\n",
    "            # Bundle all N-grams to get final representation\n",
    "            if ngram_hvs:  # Check if we have N-grams to bundle\n",
    "                sample_hv = bundle(ngram_hvs)\n",
    "            else:\n",
    "                # Fallback: just bundle all feature vectors\n",
    "                sample_hv = bundle(feature_hvs)\n",
    "\n",
    "        binarized_samples.append(sample_hv)\n",
    "\n",
    "        if show_details and sample_idx == 0:\n",
    "            # Count ones in the first sample's binary vector\n",
    "            ones_count = np.sum(sample_hv)\n",
    "            print(\n",
    "                f\"  Sample 0 binary vector: {len(sample_hv)} bits, {ones_count} ones ({ones_count / len(sample_hv):.4f})\")\n",
    "\n",
    "    # Stack all sample hypervectors into a single array\n",
    "    binarized_dataset = np.vstack(binarized_samples)\n",
    "\n",
    "    if show_details:\n",
    "        print(f\"\\nBinarized dataset shape: {binarized_dataset.shape}\")\n",
    "\n",
    "    return binarized_dataset"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T17:21:24.197792Z",
     "start_time": "2025-04-23T17:21:24.193198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wind_farm = \"C\"\n",
    "train_datasets = [55, 81]\n",
    "test_datasets = [47, 12, 4, 18, 28, 39, 66, 15, 78, 79, 30, 33, 11, 44, 49, 31, 67, 9, 91, 5, 90, 70, 35, 16,\n",
    "                 76, 8, 85, 6, 62, 36, 56, 94, 54, 43, 50, 64, 46, 65, 61, 93, 75, 41, 58, 48, 88, 57, 32, 89, 59, 63,\n",
    "                 80, 37, 29, 1, 20, 60]\n",
    "\n",
    "# [34, 7, 53, 27, 19, 77, 83, 52, 21, 2, 23, 87, 74, 86, 82]"
   ],
   "id": "feaf8573a94e6196",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T17:07:03.798682Z",
     "start_time": "2025-04-23T17:07:03.796603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create folders data_test and data_train if they do not exist\n",
    "os.makedirs(\"data_test\", exist_ok=True)\n",
    "os.makedirs(\"data_train\", exist_ok=True)"
   ],
   "id": "f44feaaab5d59a69",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T17:07:03.806459Z",
     "start_time": "2025-04-23T17:07:03.803318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "exclude_columns = [\"time_stamp\", \"asset_id\", \"id\"]\n",
    "\n",
    "\n",
    "def load_df_and_annotate_anomalies(farm, event_id):\n",
    "    path = f\"../../../data/care_to_compare/Wind Farm {farm}/datasets/{event_id}.csv\"\n",
    "    df = pd.read_csv(path, delimiter=';')\n",
    "\n",
    "    event_info = pd.read_csv(f\"../../../data/care_to_compare/Wind Farm {farm}/event_info.csv\", delimiter=';')\n",
    "\n",
    "    # Find the row where event_id = event_id\n",
    "    metadata = event_info[event_info['event_id'] == event_id]\n",
    "\n",
    "    event_label = metadata[\"event_label\"].values[0]\n",
    "    event_start_id = metadata[\"event_start_id\"].values[0]\n",
    "    event_end_id = metadata[\"event_end_id\"].values[0]\n",
    "\n",
    "    label_value = 1 if event_label == \"anomaly\" else 0\n",
    "\n",
    "    # All rows where the column \"id\" is between event_start_id and event_end_id\n",
    "    df['label'] = 0\n",
    "    df.loc[(df['id'] >= event_start_id) & (df['id'] <= event_end_id), 'label'] = label_value\n",
    "\n",
    "    # Include all columns except for the ones in exclude_columns\n",
    "    df = df[[col for col in df.columns if col not in exclude_columns]]\n",
    "\n",
    "    # Replace inf values with NaN and drop rows with NaN values\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df"
   ],
   "id": "767812d8e26efd53",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T17:07:03.817613Z",
     "start_time": "2025-04-23T17:07:03.813258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def binarize_dataset_for_training(farm, event_id, output_path):\n",
    "    # Load original dataset from file\n",
    "    df = load_df_and_annotate_anomalies(farm, event_id)\n",
    "    df = df[df['train_test'] == 'train']\n",
    "\n",
    "    # Take only 1000 rows\n",
    "    df = df[:5000]\n",
    "\n",
    "    df = df[df['status_type_id'].isin([0, 2])]\n",
    "\n",
    "    # Split into data and labels\n",
    "    X_values = df.drop(columns=['label', 'train_test', 'status_type_id'])\n",
    "    X_values = X_values.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Encode the sequence\n",
    "    final_binary_vector = binarize_dataset_with_hdc(X_values.values, show_details=False)\n",
    "\n",
    "    # Output to file using np\n",
    "    np.savetxt(f\"{output_path}/X_{farm}_{event_id}.txt\", final_binary_vector, fmt='%d')\n",
    "\n",
    "    label_df = pd.DataFrame({\n",
    "        'label': df['label'].values,\n",
    "        'status_type_id': df['status_type_id'].values,\n",
    "        'train_test': df['train_test'].values\n",
    "    })\n",
    "\n",
    "    label_df.to_csv(f\"{output_path}/y_{farm}_{event_id}.csv\", index=False)\n",
    "\n",
    "    print(f\"Done with {event_id}\")\n",
    "\n",
    "    # Calculate statistics\n",
    "    number_of_0s = np.sum(final_binary_vector == 0)\n",
    "    number_of_1s = np.sum(final_binary_vector == 1)\n",
    "\n",
    "    total_bits = len(final_binary_vector)\n",
    "\n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"Total bits: {total_bits}\")\n",
    "    print(f\"Number of 1s: {number_of_1s}\")\n",
    "    print(f\"Number of 0s: {number_of_0s}\")\n",
    "\n",
    "\n",
    "def binarize_dataset_for_testing(farm, event_id, output_path):\n",
    "    # Load original dataset from file\n",
    "    df = load_df_and_annotate_anomalies(farm, event_id)\n",
    "\n",
    "    # Only take the data that is in the prediction set\n",
    "    df = df[df['train_test'] == 'prediction']\n",
    "\n",
    "    # Split into data and labels\n",
    "    X_values = df.drop(columns=['label', 'train_test', 'status_type_id'])\n",
    "    X_values = X_values.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Encode the sequence\n",
    "    final_binary_vector = binarize_dataset_with_hdc(X_values.values)\n",
    "\n",
    "    # Output to file using np\n",
    "    np.savetxt(f\"{output_path}/X_{farm}_{event_id}.txt\", final_binary_vector, fmt='%d')\n",
    "\n",
    "    label_df = pd.DataFrame({\n",
    "        'label': df['label'].values,\n",
    "        'status_type_id': df['status_type_id'].values,\n",
    "        'train_test': df['train_test'].values\n",
    "    })\n",
    "\n",
    "    label_df.to_csv(f\"{output_path}/y_{farm}_{event_id}.csv\", index=False)\n",
    "\n",
    "    # Calculate statistics\n",
    "    total_bits = len(final_binary_vector)\n",
    "\n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"Total rows: {total_bits}\")\n",
    "    print(f\"Total columns: {len(final_binary_vector[0])}\")\n"
   ],
   "id": "6c104fdf24752f99",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T17:15:38.028872Z",
     "start_time": "2025-04-23T17:07:03.824046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset in train_datasets:\n",
    "    binarize_dataset_for_training(wind_farm, dataset, \"./data_train\")"
   ],
   "id": "89841ee82e5285d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 55\n",
      "\n",
      "Statistics:\n",
      "Total bits: 4245\n",
      "Number of 1s: 15388768\n",
      "Number of 0s: 16873232\n",
      "Done with 81\n",
      "\n",
      "Statistics:\n",
      "Total bits: 4225\n",
      "Number of 1s: 15769953\n",
      "Number of 0s: 16340047\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:42:04.936666Z",
     "start_time": "2025-04-23T17:21:27.910180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset in test_datasets:\n",
    "    binarize_dataset_for_testing(wind_farm, dataset, \"./data_test\")"
   ],
   "id": "a4f5f329094e459e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics:\n",
      "Total rows: 1577\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 3547\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 3889\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1728\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 3358\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1167\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1807\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2737\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 586\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 721\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 3551\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 3313\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 4021\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 10443\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1606\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2029\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 8929\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 3469\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 4048\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 667\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2320\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 3478\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1207\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2304\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 822\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2242\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1585\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2305\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1032\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2168\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2296\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2737\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 3025\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2593\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 3457\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1873\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2573\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 3358\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 3025\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 3313\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 3601\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 4258\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 1873\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2737\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2881\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2449\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2449\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2737\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2737\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2305\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2353\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2449\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2305\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2161\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2593\n",
      "Total columns: 7600\n",
      "\n",
      "Statistics:\n",
      "Total rows: 2449\n",
      "Total columns: 7600\n"
     ]
    }
   ],
   "execution_count": 43
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
