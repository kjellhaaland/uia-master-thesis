{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Experiment description\n",
    "\n",
    "The goal of this notebook is to preprocess all datasets in Wind Farm C to a binary format that can be used for training a TM Classifier."
   ],
   "id": "e85b94674bcffa5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T15:58:42.675743Z",
     "start_time": "2025-04-24T15:58:42.665941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import all required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ],
   "id": "a7fa0f63187f7de2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T15:58:42.684152Z",
     "start_time": "2025-04-24T15:58:42.680438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wind_farm = \"C\"\n",
    "train_datasets = [55, 81, 47, 12, 4, 18, 28, 39, 66, 15, 8, 79, 30]\n",
    "test_datasets = [55, 81, 47, 12, 4, 18, 28, 39, 66, 15, 8, 79, 30, 33, 11, 44, 49, 31, 67, 9, 91, 5, 90, 70, 35, 16,\n",
    "                 76, 8, 85, 6, 62, 36, 56, 94, 54, 43, 50, 64, 46, 65, 61, 93, 75, 41, 58, 48, 88, 57, 32, 89, 59, 63,\n",
    "                 80, 37, 29, 1, 20, 60]\n"
   ],
   "id": "7de2a6ba82e2aea6",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T15:58:42.701012Z",
     "start_time": "2025-04-24T15:58:42.695027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create folders data_test and data_train if they do not exist\n",
    "os.makedirs(\"data_test\", exist_ok=True)\n",
    "os.makedirs(\"data_train\", exist_ok=True)"
   ],
   "id": "33fcab2e88c188e5",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T15:58:42.713757Z",
     "start_time": "2025-04-24T15:58:42.708241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "exclude_columns = [\"time_stamp\", \"asset_id\", \"id\"]\n",
    "\n",
    "\n",
    "def load_df_and_annotate_anomalies(farm, event_id):\n",
    "    path = f\"../../../data/care_to_compare/Wind Farm {farm}/datasets/{event_id}.csv\"\n",
    "    df = pd.read_csv(path, delimiter=';')\n",
    "\n",
    "    event_info = pd.read_csv(f\"../../../data/care_to_compare/Wind Farm {farm}/event_info.csv\", delimiter=';')\n",
    "\n",
    "    # Find the row where event_id = event_id\n",
    "    metadata = event_info[event_info['event_id'] == event_id]\n",
    "\n",
    "    event_label = metadata[\"event_label\"].values[0]\n",
    "    event_start_id = metadata[\"event_start_id\"].values[0]\n",
    "    event_end_id = metadata[\"event_end_id\"].values[0]\n",
    "\n",
    "    label_value = 1 if event_label == \"anomaly\" else 0\n",
    "\n",
    "    # All rows where the column \"id\" is between event_start_id and event_end_id\n",
    "    df['label'] = 0\n",
    "    df.loc[(df['id'] >= event_start_id) & (df['id'] <= event_end_id), 'label'] = label_value\n",
    "\n",
    "    # Include all columns except for the ones in exclude_columns\n",
    "    df = df[[col for col in df.columns if col not in exclude_columns]]\n",
    "\n",
    "    # Remove columns with suffixes in exclude_columns_with_suffix\n",
    "    # df = df[[col for col in df.columns if not col.endswith('_avg')]]\n",
    "    # df = df[[col for col in df.columns if not col.endswith('_min')]]\n",
    "    # df = df[[col for col in df.columns if not col.endswith('_std')]]\n",
    "\n",
    "    # Replace inf values with NaN and drop rows with NaN values\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df"
   ],
   "id": "890df7a6f9bbbd18",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T15:58:42.731979Z",
     "start_time": "2025-04-24T15:58:42.727577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_min_max(farm, dataset_ids):\n",
    "    # For each column get the min and max value\n",
    "    min_max_values = {}\n",
    "\n",
    "    for dataset_id in dataset_ids:\n",
    "        df = load_df_and_annotate_anomalies(farm, dataset_id)\n",
    "\n",
    "        for col in df.columns:\n",
    "            min_val = df[col].min()\n",
    "            max_val = df[col].max()\n",
    "\n",
    "            if col not in min_max_values:\n",
    "                min_max_values[col] = (min_val, max_val)\n",
    "            else:\n",
    "                current_min, current_max = min_max_values[col]\n",
    "                min_max_values[col] = (min(min_val, current_min), max(max_val, current_max))\n",
    "\n",
    "    return min_max_values"
   ],
   "id": "a044eef9e1b7a0f8",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T15:58:42.752411Z",
     "start_time": "2025-04-24T15:58:42.742213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_to_bit_integers_log(df, minmax, bits_per_column=8):\n",
    "    normalized_df = df.copy()\n",
    "\n",
    "    for col in df.columns:\n",
    "        min_val = minmax[col][0]\n",
    "        max_val = minmax[col][1]\n",
    "\n",
    "        # If all values are 0, then set all values to 0\n",
    "        if min_val == 0 and max_val == 0:\n",
    "            normalized_df[col] = 0\n",
    "            continue\n",
    "\n",
    "        shifted = normalized_df[col] - min_val + 1\n",
    "        log_data = np.log1p(shifted)\n",
    "\n",
    "        global_log_min = np.log1p(1)  # log1p(1) because shift guarantees min = 1\n",
    "        global_log_max = np.log1p(max_val - min_val + 1)  # Max in transformed space\n",
    "\n",
    "        normalized_df[col] = (log_data - global_log_min) / (global_log_max - global_log_min) * (\n",
    "                (2 ** bits_per_column) - 1)\n",
    "\n",
    "    # Convert the normalized values to integers\n",
    "    int_df = normalized_df.astype(int)\n",
    "\n",
    "    # Flatten each row into an array of 10-bit integers\n",
    "    int_arrays = int_df.apply(lambda row: row.values.flatten(), axis=1).tolist()\n",
    "\n",
    "    # Represent each cell as a 10-bit integer string\n",
    "    bin_arrays = [[f\"{cell:0{bits_per_column}b}\" for cell in row] for row in int_arrays]\n",
    "\n",
    "    # Split each 10-bit integer string into individual integers for each row\n",
    "    # preserve the columns of bin_arrays\n",
    "    bin_int_arrays = [[int(cell) for cell in list(''.join(row))] for row in bin_arrays]\n",
    "\n",
    "    # Convert to numpy array\n",
    "    int_arrays = np.array(bin_int_arrays)\n",
    "\n",
    "    return int_arrays\n",
    "\n",
    "\n",
    "def convert_to_bit_integers(df, minmax, bits_per_column=8):\n",
    "    normalized_df = df.copy()\n",
    "\n",
    "    for col in df.columns:\n",
    "        min_val = minmax[col][0]\n",
    "        max_val = minmax[col][1]\n",
    "\n",
    "        # If all values are the same, set all to 0\n",
    "        if min_val == max_val:\n",
    "            normalized_df[col] = 0\n",
    "            continue\n",
    "\n",
    "        # Min-Max normalization\n",
    "        normalized_df[col] = (normalized_df[col] - min_val) / (max_val - min_val)\n",
    "        normalized_df[col] *= (2 ** bits_per_column) - 1\n",
    "\n",
    "    # Convert the normalized values to integers\n",
    "    int_df = normalized_df.astype(int)\n",
    "\n",
    "    # Flatten each row into an array of bit integers\n",
    "    int_arrays = int_df.apply(lambda row: row.values.flatten(), axis=1).tolist()\n",
    "\n",
    "    # Convert each integer to a binary string of fixed length\n",
    "    bin_arrays = [[f\"{cell:0{bits_per_column}b}\" for cell in row] for row in int_arrays]\n",
    "\n",
    "    # Convert binary strings into individual bits\n",
    "    bin_int_arrays = [[int(bit) for bit in ''.join(row)] for row in bin_arrays]\n",
    "\n",
    "    # Convert to numpy array\n",
    "    int_arrays = np.array(bin_int_arrays)\n",
    "\n",
    "    return int_arrays"
   ],
   "id": "383106dfae1e5bf4",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T15:58:42.765812Z",
     "start_time": "2025-04-24T15:58:42.760377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def binarize_dataset_for_training(farm, event_id, output_path, min_max_values, bits_per_column=8):\n",
    "    # Load original dataset from file\n",
    "    df = load_df_and_annotate_anomalies(farm, event_id)\n",
    "    df = df[df['train_test'] == 'train']\n",
    "\n",
    "    df = df[df['status_type_id'].isin([0, 2])]\n",
    "\n",
    "    # Take a sample of 2000 rows\n",
    "    df = df.sample(n=2000, random_state=42)\n",
    "\n",
    "    # Split into data and labels\n",
    "    X_values = df.drop(columns=['label', 'train_test', 'status_type_id'])\n",
    "\n",
    "    #X_values = X_values.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    X_values_bin = convert_to_bit_integers(X_values, min_max_values, bits_per_column).astype(np.uint32)\n",
    "\n",
    "    # Output to file using np\n",
    "    np.savetxt(f\"{output_path}/X_{farm}_{event_id}.txt\", X_values_bin, fmt='%d')\n",
    "\n",
    "    label_df = pd.DataFrame({\n",
    "        'label': df['label'].values,\n",
    "        'status_type_id': df['status_type_id'].values,\n",
    "        'train_test': df['train_test'].values\n",
    "    })\n",
    "\n",
    "    label_df.to_csv(f\"{output_path}/y_{farm}_{event_id}.csv\", index=False)\n",
    "\n",
    "    print(f\"Done with {event_id}: {X_values_bin.shape}\")\n",
    "\n",
    "\n",
    "def binarize_dataset_for_testing(farm, event_id, output_path, min_max_values, bits_per_column=8):\n",
    "    # Load original dataset from file\n",
    "    df = load_df_and_annotate_anomalies(farm, event_id)\n",
    "\n",
    "    # Split into data and labels\n",
    "    X_values = df.drop(columns=['label', 'train_test', 'status_type_id'])\n",
    "    #X_values = X_values.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    X_values_bin = convert_to_bit_integers(X_values, min_max_values, bits_per_column).astype(np.uint32)\n",
    "\n",
    "    # Output to file using np\n",
    "    np.savetxt(f\"{output_path}/X_{farm}_{event_id}.txt\", X_values_bin, fmt='%d')\n",
    "\n",
    "    label_df = pd.DataFrame({\n",
    "        'label': df['label'].values,\n",
    "        'status_type_id': df['status_type_id'].values,\n",
    "        'train_test': df['train_test'].values\n",
    "    })\n",
    "\n",
    "    label_df.to_csv(f\"{output_path}/y_{farm}_{event_id}.csv\", index=False)\n",
    "\n",
    "    print(f\"Done with {event_id}: {X_values_bin.shape}\")\n"
   ],
   "id": "67a9eb610d672ee3",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T15:58:52.407082Z",
     "start_time": "2025-04-24T15:58:42.782275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_max_values_dict = calculate_min_max(wind_farm, test_datasets)\n",
    "\n",
    "# Save to file\n",
    "with open(\"min_max_values.txt\", \"w\") as f:\n",
    "    for key, value in min_max_values_dict.items():\n",
    "        f.write(f\"{key}: {value}\\n\")"
   ],
   "id": "b140f91bb2a74030",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m min_max_values_dict \u001B[38;5;241m=\u001B[39m \u001B[43mcalculate_min_max\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwind_farm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_datasets\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Save to file\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin_max_values.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "Cell \u001B[0;32mIn[21], line 6\u001B[0m, in \u001B[0;36mcalculate_min_max\u001B[0;34m(farm, dataset_ids)\u001B[0m\n\u001B[1;32m      3\u001B[0m min_max_values \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset_id \u001B[38;5;129;01min\u001B[39;00m dataset_ids:\n\u001B[0;32m----> 6\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mload_df_and_annotate_anomalies\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfarm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_id\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mcolumns:\n\u001B[1;32m      9\u001B[0m         min_val \u001B[38;5;241m=\u001B[39m df[col]\u001B[38;5;241m.\u001B[39mmin()\n",
      "Cell \u001B[0;32mIn[20], line 6\u001B[0m, in \u001B[0;36mload_df_and_annotate_anomalies\u001B[0;34m(farm, event_id)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_df_and_annotate_anomalies\u001B[39m(farm, event_id):\n\u001B[1;32m      5\u001B[0m     path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../../../data/care_to_compare/Wind Farm \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfarm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/datasets/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mevent_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 6\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelimiter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m;\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m     event_info \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../../../data/care_to_compare/Wind Farm \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfarm\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/event_info.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, delimiter\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m;\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;66;03m# Find the row where event_id = event_id\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/GitHub/uia-master-thesis/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/uia-master-thesis/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[0;32m--> 626\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/uia-master-thesis/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m   1916\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[1;32m   1917\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1918\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[1;32m   1919\u001B[0m     (\n\u001B[1;32m   1920\u001B[0m         index,\n\u001B[1;32m   1921\u001B[0m         columns,\n\u001B[1;32m   1922\u001B[0m         col_dict,\n\u001B[0;32m-> 1923\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[1;32m   1924\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[1;32m   1925\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1926\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m   1927\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/Documents/GitHub/uia-master-thesis/.venv/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[0;32m--> 234\u001B[0m         chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_low_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    235\u001B[0m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[1;32m    236\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[0;32mparsers.pyx:838\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mparsers.pyx:905\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mparsers.pyx:874\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mparsers.pyx:891\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mparsers.pyx:2053\u001B[0m, in \u001B[0;36mpandas._libs.parsers.raise_parser_error\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m<frozen codecs>:331\u001B[0m, in \u001B[0;36mgetstate\u001B[0;34m(self)\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T16:00:48.663712Z",
     "start_time": "2025-04-24T16:00:48.654189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read min_max_values from file\n",
    "\n",
    "min_max_values_dict = {}\n",
    "\n",
    "with open(\"min_max_values.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        key, value = line.strip().split(\": \")\n",
    "        min_max_values_dict[key] = tuple(map(any, value.strip(\"()\").split(\",\")))\n",
    "\n"
   ],
   "id": "508e10a511998787",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T16:00:52.353939Z",
     "start_time": "2025-04-24T16:00:52.351752Z"
    }
   },
   "cell_type": "code",
   "source": "bits = 5",
   "id": "a824517b052b4a92",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T17:09:31.655055Z",
     "start_time": "2025-04-24T16:00:52.813718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset in test_datasets:\n",
    "    binarize_dataset_for_testing(wind_farm, dataset, \"./data_test\", min_max_values_dict, bits)"
   ],
   "id": "e879c62b0a552f43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 55: (55753, 4760)\n",
      "Done with 81: (53932, 4760)\n",
      "Done with 47: (53993, 4760)\n",
      "Done with 12: (56107, 4760)\n",
      "Done with 4: (56449, 4760)\n",
      "Done with 18: (52848, 4760)\n",
      "Done with 28: (55918, 4760)\n",
      "Done with 39: (53727, 4760)\n",
      "Done with 66: (53503, 4760)\n",
      "Done with 15: (54433, 4760)\n",
      "Done with 8: (54802, 4760)\n",
      "Done with 79: (53281, 4760)\n",
      "Done with 30: (56111, 4760)\n",
      "Done with 33: (55873, 4760)\n",
      "Done with 11: (56437, 4760)\n",
      "Done with 44: (63003, 4760)\n",
      "Done with 49: (53014, 4760)\n",
      "Done with 31: (54589, 4760)\n",
      "Done with 67: (61489, 4760)\n",
      "Done with 9: (56029, 4760)\n",
      "Done with 91: (56608, 4760)\n",
      "Done with 5: (52795, 4760)\n",
      "Done with 90: (54880, 4760)\n",
      "Done with 70: (56038, 4760)\n",
      "Done with 35: (52615, 4760)\n",
      "Done with 16: (53568, 4760)\n",
      "Done with 76: (52086, 4760)\n",
      "Done with 8: (54802, 4760)\n",
      "Done with 85: (52417, 4760)\n",
      "Done with 6: (54865, 4760)\n",
      "Done with 62: (53448, 4760)\n",
      "Done with 36: (54440, 4760)\n",
      "Done with 56: (53416, 4760)\n",
      "Done with 94: (54865, 4760)\n",
      "Done with 54: (55585, 4760)\n",
      "Done with 43: (55153, 4760)\n",
      "Done with 50: (55153, 4760)\n",
      "Done with 64: (54433, 4760)\n",
      "Done with 46: (55133, 4760)\n",
      "Done with 65: (55918, 4760)\n",
      "Done with 61: (55585, 4760)\n",
      "Done with 93: (55873, 4760)\n",
      "Done with 75: (56161, 4760)\n",
      "Done with 41: (55666, 4760)\n",
      "Done with 58: (54433, 4760)\n",
      "Done with 48: (55297, 4760)\n",
      "Done with 88: (55441, 4760)\n",
      "Done with 57: (55009, 4760)\n",
      "Done with 32: (55009, 4760)\n",
      "Done with 89: (54577, 4760)\n",
      "Done with 59: (54865, 4760)\n",
      "Done with 63: (54865, 4760)\n",
      "Done with 80: (54913, 4760)\n",
      "Done with 37: (53713, 4760)\n",
      "Done with 29: (54865, 4760)\n",
      "Done with 1: (53569, 4760)\n",
      "Done with 20: (54001, 4760)\n",
      "Done with 60: (54433, 4760)\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T17:10:38.168140Z",
     "start_time": "2025-04-24T17:09:31.703711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset in train_datasets:\n",
    "    binarize_dataset_for_training(wind_farm, dataset, \"./data_train\", min_max_values_dict, bits)"
   ],
   "id": "a312821be058472e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 55: (2000, 4760)\n",
      "Done with 81: (2000, 4760)\n",
      "Done with 47: (2000, 4760)\n",
      "Done with 12: (2000, 4760)\n",
      "Done with 4: (2000, 4760)\n",
      "Done with 18: (2000, 4760)\n",
      "Done with 28: (2000, 4760)\n",
      "Done with 39: (2000, 4760)\n",
      "Done with 66: (2000, 4760)\n",
      "Done with 15: (2000, 4760)\n",
      "Done with 8: (2000, 4760)\n",
      "Done with 79: (2000, 4760)\n",
      "Done with 30: (2000, 4760)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "908b5dac52bb60be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
